-- MySQL dump 10.13  Distrib 8.0.27, for Linux (x86_64)
--
-- Host: localhost    Database: mini
-- ------------------------------------------------------
-- Server version	8.0.27

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!50503 SET NAMES utf8mb4 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `ver0_affiliation`
--

DROP TABLE IF EXISTS `ver0_affiliation`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `ver0_affiliation` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(128) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=133 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `ver0_affiliation`
--

LOCK TABLES `ver0_affiliation` WRITE;
/*!40000 ALTER TABLE `ver0_affiliation` DISABLE KEYS */;
INSERT INTO `ver0_affiliation` VALUES (1,'Vrije Universiteit Brussel'),(2,'Interdisciplinary Institute for Broadband Technology IBBT'),(3,'University of Southern California'),(4,'Saarland University'),(5,'Estimating Noise from Noisy Speech Features with a Monte Carlo'),(6,'Spoken Language Systems'),(7,'Ecole Polytechnique Federale de Lausanne'),(8,'Idiap Research Institute - Rue Marconi'),(9,'LIMSI-CNRS'),(10,'SRI International'),(11,'University of New South Wales'),(12,'University of Eastern Finland'),(13,'Cambridge University Engineering Department'),(14,'CLUL'),(15,'INESC-ID Lisboa'),(16,'Berlin University of Technology'),(17,'IST Lisboa'),(18,'Kyoto University'),(19,'Zhejiang University'),(20,'Jiangxi Normal University'),(21,'Baum-Welch statistics. Our approach and Kenny’s approach'),(22,'Université de Moncton'),(23,'postfocal phrasal heads in Italian'),(24,'University of Salento'),(25,'The University of Texas at Dallas'),(26,'Radboud University Nijmegen'),(27,'Hokkaido University'),(28,'Toyohashi University of Technology'),(29,'University of Lübeck'),(30,'Tokyo Institute of Technology'),(31,'Tokyo City University'),(32,'Hidden Markov Convolutive Mixture Model'),(33,'The University of Tokyo'),(34,'Sophia University'),(35,'with Cross-Corpora Evaluation'),(36,'Cambridge University'),(37,'Trumpington St. Cambridge University'),(38,'Beuth University'),(39,'Universität Potsdam'),(40,'University of Edinburgh'),(41,'LORIA CNRS UMR7503'),(42,'German Consonants'),(43,'Tongji University'),(44,'Johns Hopkins University'),(45,'Language Technologies Institute'),(46,'Carnegie Mellon University'),(47,'František Grézl'),(48,'Brno University of Technology'),(49,'Fondazione Bruno Kessler'),(50,'Northwestern Polytechnical University'),(51,'Crowdee: Mobile Crowdsourcing Micro-task Platform'),(52,'Telekom Innovation Laboratories'),(53,'Friedrich-Alexander Universität (FAU)'),(54,'Aalto University'),(55,'Indian Institute of Technology Guwahati'),(56,'Aristotle University of Thessaloniki'),(57,'Boğaziçi University'),(58,'Erinç Dikici'),(59,'The University of Hong Kong'),(60,'Voci Technologies Inc'),(61,'Neural-Network Turn-Taking Models'),(62,'University of Western Sydney'),(63,'Ludwig-Maximilians-Universität München'),(64,'Crowdsourcing Paradigm'),(65,'Georgia Institute of Technology'),(66,'Tsinghua University'),(67,'Kore University of Enna'),(68,'Microsoft Corporation'),(69,'University of Gothenburg'),(70,'IBM T. J. Watson Research Center'),(71,'CNRS'),(72,'Univ. Grenoble Alpes'),(73,'The Chinese University of Hong Kong'),(74,'Chinese Academy of Sciences'),(75,'Key Laboratory of Human-Machine Intelligence-Synergy Systems'),(76,'University of California'),(77,'Bielefeld University'),(78,'Online Lombard-adaptation in incremental speech synthesis'),(79,'University of Michigan'),(80,'Using Real-Time Magnetic Resonance Imaging'),(81,'Haskins Laboratories'),(82,'MIT Lincoln Laboratory'),(83,'Queensland University of Technology'),(84,'Yamaha Corporation'),(85,'Sequence-To-Sequence LSTM-based Autoencoders'),(86,'Academia Sinica'),(87,'National Taiwan Normal University'),(88,'Waseda University'),(89,'University of Sheffield'),(90,'the World’s First Commercial Biomimetic Robot'),(91,'Intel Corporation'),(92,'{trmal'),(93,'Incheon National University'),(94,'Korea University'),(95,'Chubu University'),(96,'Nuance Communications'),(97,'Stanford University'),(98,'South China University of Technology'),(99,'Microsoft Research Asia'),(100,'Shanghai Jiao Tong University'),(101,'University of Stuttgart'),(102,'Frisian Studies and General Linguistics'),(103,'University of Kiel'),(104,'University of Southern Denmark'),(105,'University of Toronto'),(106,'City University of New York'),(107,'Idiap Research Institute'),(108,'Ellipsis Health, Inc'),(109,'Apple Inc'),(110,'The Johns Hopkins University'),(111,'University of Illinois at Urbana-Champaign'),(112,'Delft University of Technology'),(113,'National Chiao Tung University'),(114,'The Hong Kong Polytechnic University'),(115,'Beijing Language and Culture University'),(116,'University College London'),(117,'for Incremental Neural TTS'),(118,'The University of Edinburgh'),(119,'EURECOM'),(120,'Northwestern University'),(121,'Qatar Computing Research Institute'),(122,'KanarI AI'),(123,'Google Inc'),(124,'National University of Singapore (NUS)'),(125,'University of Bremen'),(126,'Singapore University of Technology and Design (SUTD)'),(127,'Logitech Europe S.A'),(128,'École Polytechnique Fédérale de Lausanne'),(129,'prosodic boundaries which separate the potentially assimilated'),(130,'Nanyang Technological University'),(131,'Trinity College Dublin'),(132,'The LF Model');
/*!40000 ALTER TABLE `ver0_affiliation` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `ver0_author`
--

DROP TABLE IF EXISTS `ver0_author`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `ver0_author` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(128) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=328 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `ver0_author`
--

LOCK TABLES `ver0_author` WRITE;
/*!40000 ALTER TABLE `ver0_author` DISABLE KEYS */;
INSERT INTO `ver0_author` VALUES (1,'Wesley Mattheyses'),(2,'Lukas Latacz'),(3,'Werner Verhelst'),(4,'Yohei Kawaguchi'),(5,'Masahito Togami'),(6,'Yasunari Obuchi'),(7,'Erik Bresch'),(8,'Athanasios Katsamanis'),(9,'Louis Goldstein'),(10,'Shrikanth S. Narayanan'),(11,'Friedrich Faubel'),(12,'Dietrich Klakow'),(13,'Alfred Dielmann'),(14,'Giulia Garau'),(15,'Hervé Bourlard'),(16,'Dimitra Vergyri'),(17,'Lori Lamel'),(18,'Jean-Luc Gauvain'),(19,'Ville Hautamäki'),(20,'Tomi Kinnunen'),(21,'Mohaddeseh Nosratighods'),(22,'Kong Aik Lee'),(23,'Bin Ma'),(24,'Haizhou Li'),(25,'Matt Shannon'),(26,'William Byrne'),(27,'Oscar Koller'),(28,'Alberto Abad'),(29,'Isabel Trancoso'),(30,'Céu Viana'),(31,'Shinsuke Mori'),(32,'Graham Neubig'),(33,'Taichi Asami'),(34,'Narichika Nomoto'),(35,'Satoshi Kobashikawa'),(36,'Yoshikazu Yamaguchi'),(37,'Hirokazu Masataki'),(38,'Satoshi Takahashi'),(39,'Zhenchun Lei'),(40,'Yingchun Yang'),(41,'Md Foezur Rahman Chowdhury'),(42,'Sid-Ahmed Selouani'),(43,'Douglas O\'Shaughnessy'),(44,'Giuliano Bocci'),(45,'Cinzia Avesani'),(46,'Bianca Sisinni'),(47,'Mirko Grimaldi'),(48,'Azar Taufique'),(49,'Kumaran Vijayasankar'),(50,'Wooil Kim'),(51,'John H. L. Hansen'),(52,'Marco Tacca'),(53,'Andrea Fumagalli'),(54,'Miranti Indar Mandasari'),(55,'Mitchell McLaren'),(56,'David A. van Leeuwen'),(57,'Yurie Iribe'),(58,'Takurou Mori'),(59,'Kouichi Katsurada'),(60,'Goh Kawai'),(61,'Tsuneo Nitta'),(62,'Florian Müller'),(63,'Alfred Mertins'),(64,'Ryo Yokoyama'),(65,'Yu Nasu'),(66,'Koichi Shinoda'),(67,'Koji Iwano'),(68,'Kota Yoshizato'),(69,'Hirokazu Kameoka'),(70,'Daisuke Saito'),(71,'Shigeki Sagayama'),(72,'Takayuki Arai'),(73,'Daniel Bone'),(74,'Chi-Chun Lee'),(75,'Yongqiang Wang'),(76,'Mark J. F. Gales'),(77,'Hansjörg Mixdorff'),(78,'Oliver Niebuhr'),(79,'James M. Scobbie'),(80,'Alice Turk'),(81,'Christian Geng'),(82,'Simon King'),(83,'Robin Lickley'),(84,'Korin Richmond'),(85,'Shinji Maeda'),(86,'Yves Laprie'),(87,'Caitlin Smith'),(88,'Adam Lammert'),(89,'Fabian Brackhane'),(90,'Jürgen Trouvain'),(91,'Nguyen Duc Duy'),(92,'Masayuki Suzuki'),(93,'Nobuaki Minematsu'),(94,'Keikichi Hirose'),(95,'Hongwei Ding'),(96,'Rüdiger Hoffmann'),(97,'Keith Kintzley'),(98,'Aren Jansen'),(99,'Hynek Hermansky'),(100,'William Chan'),(101,'Ian Lane'),(102,'František Grézl'),(103,'Martin Karafiát'),(104,'Mirco Ravanelli'),(105,'Maurizio Omologo'),(106,'Peng Yang'),(107,'Cheung-Chi Leung'),(108,'Lei Xie'),(109,'Babak Naderi'),(110,'Tim Polzehl'),(111,'André Beyer'),(112,'Tibor Pilz'),(113,'Sebastian Möller'),(114,'Emma Jokinen'),(115,'Ulpu Remes'),(116,'Marko Takanen'),(117,'Kalle Palomäki'),(118,'Mikko Kurimo'),(119,'Paavo Alku'),(120,'S. Shahnawazuddin'),(121,'Rohit Sinha'),(122,'Angelos Lengeris'),(123,'Katerina Nicolaidis'),(124,'Erinç Dikici'),(125,'Murat Saraçlar'),(126,'Fei Chen'),(127,'Sharon W. K. Wong'),(128,'Lena L. N. Wong'),(129,'Kornel Laskowski'),(130,'Simone Simonetti'),(131,'Jeesun Kim'),(132,'Chris Davis'),(133,'Gábor Gosztolya'),(134,'Mako Ishida'),(135,'Thomas Kisler'),(136,'Florian Schiel'),(137,'Uwe D. Reichel'),(138,'Christoph Draxler'),(139,'Ruhi Sarikaya'),(140,'Ina Wechsung'),(141,'Friedemann Köster'),(142,'Zhen Huang'),(143,'Jinyu Li'),(144,'Sabato Marco Siniscalchi'),(145,'I-Fan Chen'),(146,'Ji Wu'),(147,'Chin-Hui Lee'),(148,'RaviShankar Prasad'),(149,'B. Yegnanarayana'),(150,'Yasuko Nagano-Madsen'),(151,'Jia Cui'),(152,'George Saon'),(153,'Bhuvana Ramabhadran'),(154,'Brian Kingsbury'),(155,'Diandra Fabre'),(156,'Thomas Hueber'),(157,'Florent Bocquelet'),(158,'Pierre Badin'),(159,'Rongfeng Su'),(160,'Xurong Xie'),(161,'Xunying Liu'),(162,'Lan Wang'),(163,'Jody Kreiman'),(164,'Soo Jin Park'),(165,'Patricia A. Keating'),(166,'Abeer Alwan'),(167,'Sebastian Rottschäfer'),(168,'Hendrik Buschmeier'),(169,'Herwin van Welbergen'),(170,'Stefan Kopp'),(171,'Duc Le'),(172,'Emily Mower Provost'),(173,'Yongwan Lim'),(174,'Sajan Goud Lingala'),(175,'Asterios Toutios'),(176,'Krishna S. Nayak'),(177,'Adam C. Lammert'),(178,'Christine H. Shadle'),(179,'Thomas F. Quatieri'),(180,'Ahilan Kanagasundaram'),(181,'David Dean'),(182,'Sridha Sridharan'),(183,'Clinton Fookes'),(184,'Ivan Himawan'),(185,'Ryunosuke Daido'),(186,'Yuji Hisaminato'),(187,'Tanner Sorensen'),(188,'Johannes Töger'),(189,'Vincent Wan'),(190,'Yannis Agiomyrgiannakis'),(191,'Hanna Silen'),(192,'Jakub Vít'),(193,'Ying-Wen Chen'),(194,'Kuan-Yu Chen'),(195,'Hsin-Min Wang'),(196,'Berlin Chen'),(197,'Win Thuzar Kyaw'),(198,'Yoshinori Sagisaka'),(199,'Roger K. Moore'),(200,'Ben Mitchinson'),(201,'Georg Stemmer'),(202,'Munir Georges'),(203,'Joachim Hofer'),(204,'Piotr Rozen'),(205,'Josef Bauer'),(206,'Jakub Nowicki'),(207,'Tobias Bocklet'),(208,'Hannah R. Colett'),(209,'Ohad Falik'),(210,'Michael Deisher'),(211,'Sylvia J. Downing'),(212,'Jan Trmal'),(213,'Matthew Wiesner'),(214,'Vijayaditya Peddinti'),(215,'Xiaohui Zhang'),(216,'Pegah Ghahremani'),(217,'Yiming Wang'),(218,'Vimal Manohar'),(219,'Hainan Xu'),(220,'Daniel Povey'),(221,'Sanjeev Khudanpur'),(222,'Suwon Shon'),(223,'Seongkyu Mun'),(224,'Hanseok Ko'),(225,'Shoko Tsujimura'),(226,'Kazumasa Yamamoto'),(227,'Seiichi Nakagawa'),(228,'Ishin Fukuoka'),(229,'Kazuhiko Iwata'),(230,'Tetsunori Kobayashi'),(231,'Jesús Andrés-Ferrer'),(232,'Nathan Bodenstab'),(233,'Paul Vozila'),(234,'Albert Haque'),(235,'Michelle Guo'),(236,'Prateek Verma'),(237,'Yujia Xiao'),(238,'Frank Soong'),(239,'Wenping Hu'),(240,'Jun Wang'),(241,'Jie Chen'),(242,'Dan Su'),(243,'Lianwu Chen'),(244,'Meng Yu'),(245,'Yanmin Qian'),(246,'Dong Yu'),(247,'Pavel Denisov'),(248,'Ngoc Thang Vu'),(249,'Stephanie Berger'),(250,'Margaret Zellers'),(251,'Laura Spinu'),(252,'Maida Percival'),(253,'Alexei Kochetov'),(254,'S. Pavankumar Dubagunta'),(255,'Mathew Magimai-Doss'),(256,'Tomasz Rutowski'),(257,'Amir Harati'),(258,'Yang Lu'),(259,'Elizabeth Shriberg'),(260,'Xizi Wei'),(261,'Melvyn Hunt'),(262,'Adrian Skilling'),(263,'Laureano Moro-Velazquez'),(264,'JaeJin Cho'),(265,'Shinji Watanabe'),(266,'Mark A. Hasegawa-Johnson'),(267,'Odette Scharenborg'),(268,'Heejin Kim'),(269,'Najim Dehak'),(270,'Youzhi Tu'),(271,'Man-Wai Mak'),(272,'Jen-Tzung Chien'),(273,'Yingming Gao'),(274,'Xinyu Zhang'),(275,'Yi Xu'),(276,'Jinsong Zhang'),(277,'Peter Birkholz'),(278,'Brooke Stephenson'),(279,'Laurent Besacier'),(280,'Laurent Girin'),(281,'Erfan Loweimi'),(282,'Peter Bell'),(283,'Steve Renals'),(284,'Hemlata Tak'),(285,'Jose Patino'),(286,'Andreas Nautsch'),(287,'Nicholas Evans'),(288,'Massimiliano Todisco'),(289,'Amrit Romana'),(290,'John Bandon'),(291,'Noelle Carlozzi'),(292,'Angela Roberts'),(293,'Wubo Li'),(294,'Dongwei Jiang'),(295,'Wei Zou'),(296,'Xiangang Li'),(297,'Shammur Absar Chowdhury'),(298,'Amir Hussein'),(299,'Ahmed Abdelali'),(300,'Ahmed Ali'),(301,'Rami Botros'),(302,'Tara N. Sainath'),(303,'Robert David'),(304,'Emmanuel Guzman'),(305,'Wei Li'),(306,'Yanzhang He'),(307,'Ajish K. Abraham'),(308,'V. Sivaramakrishnan'),(309,'N. Swapna'),(310,'N. Manohar'),(311,'Miran Oh'),(312,'Dani Byrd'),(313,'Pascale Fung'),(314,'Rui Liu'),(315,'Berrak Sisman'),(316,'Natalia Nessler'),(317,'Milos Cernak'),(318,'Paolo Prandoni'),(319,'Pablo Mainar'),(320,'Anne Bonneau'),(321,'Hexin Liu'),(322,'Leibny Paola García Perera'),(323,'Xinyi Zhang'),(324,'Justin Dauwels'),(325,'Andy W.H. Khong'),(326,'Suzy J. Styles'),(327,'Christer Gobl');
/*!40000 ALTER TABLE `ver0_author` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `ver0_conference`
--

DROP TABLE IF EXISTS `ver0_conference`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `ver0_conference` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(128) NOT NULL,
  `year` int NOT NULL,
  `abbr` varchar(64) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=13 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `ver0_conference`
--

LOCK TABLES `ver0_conference` WRITE;
/*!40000 ALTER TABLE `ver0_conference` DISABLE KEYS */;
INSERT INTO `ver0_conference` VALUES (1,'INTERSPEECH',2010,'interspeech'),(2,'INTERSPEECH',2011,'interspeech'),(3,'INTERSPEECH',2012,'interspeech'),(4,'INTERSPEECH',2013,'interspeech'),(5,'INTERSPEECH',2014,'interspeech'),(6,'INTERSPEECH',2015,'interspeech'),(7,'INTERSPEECH',2016,'interspeech'),(8,'INTERSPEECH',2017,'interspeech'),(9,'INTERSPEECH',2018,'interspeech'),(10,'INTERSPEECH',2019,'interspeech'),(11,'INTERSPEECH',2020,'interspeech'),(12,'INTERSPEECH',2021,'interspeech');
/*!40000 ALTER TABLE `ver0_conference` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `ver0_keyword`
--

DROP TABLE IF EXISTS `ver0_keyword`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `ver0_keyword` (
  `id` int NOT NULL AUTO_INCREMENT,
  `name` varchar(128) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=573 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `ver0_keyword`
--

LOCK TABLES `ver0_keyword` WRITE;
/*!40000 ALTER TABLE `ver0_keyword` DISABLE KEYS */;
INSERT INTO `ver0_keyword` VALUES (1,'language modeling'),(2,'speech'),(3,'audiovisual speech synthesis'),(4,'aam modeling'),(5,'direction of arrival'),(6,'direction arrival'),(7,'voice detection'),(8,'voice activity detection'),(9,'turn taking'),(10,'microphone array'),(11,'DOA'),(12,'turn take'),(13,'VAD'),(14,'conversation detection'),(15,'speech production'),(16,'realtime magnetic resonance imaging'),(17,'voice production'),(18,'articulatory modeling'),(19,'MRI'),(20,'articulatory'),(21,'noise estimation'),(22,'robust speech recognition'),(23,'monte carlo'),(24,'speech recognition'),(25,'speaker turn'),(26,'floor control'),(27,'multiparty conversation'),(28,'multiple frame feature'),(29,'conversation'),(30,'dynamic'),(31,'AMI'),(32,'non-verbal features'),(33,'turn'),(34,'hierarchical control'),(35,'dynamic bayesian network'),(36,'MAP'),(37,'accent adaptation'),(38,'GMM'),(39,'accented speech recognition'),(40,'speaker adaptation'),(41,'SRE'),(42,'IST'),(43,'NIST'),(44,'cluster'),(45,'hmm-based speech synthesis'),(46,'speech sound'),(47,'autoregressive'),(48,'decision tree clustering'),(49,'HMM'),(50,'MDL'),(51,'autoregressive hmm'),(52,'accent identification'),(53,'recognition of accented speech'),(54,'VID'),(55,'TTS'),(56,'spoken documents'),(57,'confidence measure'),(58,'spoken language'),(59,'confidence measures'),(60,'contextual coherence'),(61,'PMI'),(62,'speaker verification'),(63,'factor analysis'),(64,'PCA'),(65,'WCCN'),(66,'i-vector'),(67,'principal component analysis'),(68,'MCRA'),(69,'change point detection'),(70,'noise'),(71,'line asr'),(72,'ASR'),(73,'inference'),(74,'OCP'),(75,'CRA'),(76,'joint noise compensation'),(77,'minimum error'),(78,'bayesian on-line inference'),(79,'JAC'),(80,'minimum search window'),(81,'BOCPD'),(82,'on-line asr'),(83,'non-stationary noise tracking and estimate'),(84,'non-native phone perception'),(85,'PAM'),(86,'foreign language acquisition'),(87,'cross -'),(88,'IEEE 80211'),(89,'SNR'),(90,'differentiated maximum retry limit'),(91,'phone impact'),(92,'WER'),(93,'priority class'),(94,'forensic'),(95,'calibration'),(96,'speaker recognition'),(97,'short utterances'),(98,'short utterance'),(99,'forensics'),(100,'pronunciation training'),(101,'audio feature'),(102,'CAPT'),(103,'pronunciation'),(104,'MLN'),(105,'articulatory feature'),(106,'IPA'),(107,'ipa chart'),(108,'automatic speech recognition'),(109,'vocal tract length normalization'),(110,'VTL'),(111,'VTLN'),(112,'elastic registration'),(113,'spectral subtraction'),(114,'spectral clustering'),(115,'overlap speech detection'),(116,'cosine distance'),(117,'probabilistic model'),(118,'fujisaki model'),(119,'em algorithm'),(120,'hidden markov model'),(121,'statistical model'),(122,'speech f0 contours'),(123,'physical models of the human vocal tract'),(124,'speech science'),(125,'vowel production'),(126,'S3T'),(127,'education in acoustics'),(128,'education acoustic'),(129,'vowel space'),(130,'activation'),(131,'rating'),(132,'arousal rating'),(133,'knowledge-based'),(134,'cross-corpora'),(135,'inter-rater reliability'),(136,'multi - channel'),(137,'unsupervised'),(138,'base'),(139,'reverberant noise robustness'),(140,'AURORA4'),(141,'VTS'),(142,'vector taylor series'),(143,'training'),(144,'adaptive training'),(145,'AURORA'),(146,'segmental intonation'),(147,'F0'),(148,'perception'),(149,'prominence'),(150,'f0'),(151,'intonation'),(152,'ESPF'),(153,'discourse'),(154,'AG500'),(155,'SPF'),(156,'spontaneous speech'),(157,'EMA'),(158,'vocal tract length'),(159,'vocal-tract length'),(160,'liquids'),(161,'consonant'),(162,'syllabic consonants'),(163,'vocalization'),(164,'vowel synthesis'),(165,'historical instruments'),(166,'historical instrument'),(167,'REDIAL'),(168,'ABE'),(169,'DIAL'),(170,'ICE'),(171,'SPL'),(172,'DIA'),(173,'SPLICE'),(174,'l2 german'),(175,'vowel epenthesis'),(176,'l1 chinese'),(177,'ART'),(178,'whole-word modeling'),(179,'point process model'),(180,'keyword spotting'),(181,'spot'),(182,'CART'),(183,'phonetic timing'),(184,'GPU'),(185,'distribute optimization'),(186,'deep neural network'),(187,'distributed optimization'),(188,'MLLR'),(189,'stack'),(190,'LLR'),(191,'semi-supervised training'),(192,'CML'),(193,'neural networks'),(194,'clean condition training'),(195,'CMLLR'),(196,'feature extraction'),(197,'multilingual training'),(198,'neural network'),(199,'feature'),(200,'stacked bottle-neck'),(201,'reverberation'),(202,'multi-condition training'),(203,'dynamic time warping'),(204,'TIMIT'),(205,'ISA'),(206,'time frequency'),(207,'MIT'),(208,'intrinsic spectral analysis'),(209,'STD'),(210,'spoken term detection'),(211,'gaussian posteriorgram'),(212,'tools'),(213,'recording'),(214,'crowdsourcing'),(215,'test'),(216,'crowdsource'),(217,'study'),(218,'tool'),(219,'scalable studies'),(220,'labeling'),(221,'field tests'),(222,'mobile phone app'),(223,'mobile'),(224,'gaussian mixture model'),(225,'telephone speech'),(226,'text speech'),(227,'model'),(228,'enhancement'),(229,'intelligibility enhancement'),(230,'spectral envelope estimation'),(231,'sparse representation'),(232,'redundant dictionaries'),(233,'OMP'),(234,'redundant dictionary'),(235,'eigenvoice'),(236,'on-line adaptation'),(237,'eigenvoices'),(238,'fast adaptation'),(239,'PSTM'),(240,'l2 consonants'),(241,'identification'),(242,'PST'),(243,'VCV'),(244,'l2 acquisition'),(245,'STM'),(246,'ranking perceptron'),(247,'discriminative language modeling'),(248,'unsupervised training'),(249,'DLM'),(250,'rank'),(251,'vowel sentence'),(252,'spectral degradation'),(253,'speech intelligibility'),(254,'prediction'),(255,'STT'),(256,'radial basis functions'),(257,'radial basis function'),(258,'turn-taking'),(259,'imputation'),(260,'AUC'),(261,'deep neural networks'),(262,'exponential smoothing'),(263,'signal'),(264,'speech technology'),(265,'social signals'),(266,'adaboostmh'),(267,'technology'),(268,'perceptual restoration'),(269,'speech perception'),(270,'phonemic restoration'),(271,'restoration'),(272,'second language listening'),(273,'RES'),(274,'automatic estimation'),(275,'web service'),(276,'web interface'),(277,'BAS'),(278,'MAUS'),(279,'text-to-phoneme'),(280,'REST'),(281,'automatic segmentation'),(282,'EST'),(283,'restful web service'),(284,'syllabification'),(285,'motivation'),(286,'reliability'),(287,'assessment'),(288,'speech quality assessment'),(289,'multitask learning'),(290,'WERR'),(291,'CD-DNN-HMM'),(292,'adaptation'),(293,'MTL'),(294,'WSJ'),(295,'DNN'),(296,'ZTW'),(297,'pitch regression'),(298,'zero time windowing'),(299,'numerator of group delay function'),(300,'time windowe'),(301,'group delay function'),(302,'pitch'),(303,'pause'),(304,'prosodic phrasing'),(305,'interlanguage'),(306,'swedish'),(307,'japanese'),(308,'IARPA'),(309,'ARPA'),(310,'RPA'),(311,'RNN'),(312,'multitask training'),(313,'biofeedback'),(314,'tongue'),(315,'segmentation'),(316,'speech therapy'),(317,'ANN'),(318,'ultrasound imaging'),(319,'parameter'),(320,'generalized variable parameter hmm'),(321,'distance feature'),(322,'bottleneck features'),(323,'GVP'),(324,'voice quality'),(325,'intraspeaker variability'),(326,'quality'),(327,'incremental processing'),(328,'speech synthesis'),(329,'effect'),(330,'lombard effect'),(331,'interactive systems'),(332,'dialog system'),(333,'aphasia'),(334,'domain transfer'),(335,'aphasiabank'),(336,'acoustic modeling'),(337,'acoustic classification'),(338,'out-of-domain adaptation'),(339,'law'),(340,'USC'),(341,'real-time mri'),(342,'articulatory difficulty'),(343,'fitts’ law'),(344,'i-vectors'),(345,'partition'),(346,'SUV'),(347,'LDA'),(348,'GPLDA'),(349,'PLDA'),(350,'utterance partitioning'),(351,'EER'),(352,'plda'),(353,'speech analysis'),(354,'SWIPE'),(355,'speech speech'),(356,'speech processing'),(357,'fundamental frequency (f0)'),(358,'fundamental'),(359,'YIN'),(360,'DIO'),(361,'RMA'),(362,'magnetic resonance imaging'),(363,'word selection'),(364,'text-to-speech synthesis'),(365,'LSTM'),(366,'unit-selection'),(367,'SDR'),(368,'query model'),(369,'pseudo relevance feedback'),(370,'significant words'),(371,'word'),(372,'textural features'),(373,'voice source features'),(374,'cross-modal sentiment correlation'),(375,'phonation difference'),(376,'phonation differences'),(377,'mammalian vocalisation'),(378,'INT'),(379,'SPE'),(380,'vocal synthesis'),(381,'biomimetic robot'),(382,'TER'),(383,'INTERSPEECH'),(384,'miro'),(385,'synthesis'),(386,'neural network hardware'),(387,'SRAM'),(388,'language model'),(389,'4MB'),(390,'hardware'),(391,'natural language understanding'),(392,'DSP'),(393,'BAB'),(394,'TWV'),(395,'KWS'),(396,'BABEL'),(397,'keyword search'),(398,'ATWV'),(399,'iarpa babel'),(400,'openkws'),(401,'denoising autoencoder'),(402,'EDA'),(403,'autoencoder'),(404,'AED'),(405,'AEDA'),(406,'denoise autoencoder'),(407,'unsupervised domain adaptation'),(408,'domain mismatch'),(409,'lecture data'),(410,'dtw'),(411,'slide-speech alignment'),(412,'DTW'),(413,'discourse analysis'),(414,'conversational speech'),(415,'prosody'),(416,'analysis'),(417,'NCE'),(418,'100K'),(419,'KLD'),(420,'ESL'),(421,'pronunciation quality evaluation'),(422,'evaluation'),(423,'GOP'),(424,'deep nerual network'),(425,'computer-aided language learning'),(426,'source separation'),(427,'source'),(428,'speaker extraction'),(429,'PESQ'),(430,'end-to-end asr'),(431,'overlapped speech'),(432,'end end asr'),(433,'pitch range'),(434,'views'),(435,'subscribers'),(436,'intensity variation'),(437,'charisma'),(438,'youtube'),(439,'subscriber'),(440,'likes'),(441,'articulation'),(442,'fricative'),(443,'secondary palatalization'),(444,'romanian'),(445,'fricatives'),(446,'convolutional neural networks'),(447,'raw-waveform modelling'),(448,'computational paralinguistics'),(449,'source-filter decomposition'),(450,'raw waveform modelling'),(451,'articulatory modelling'),(452,'affective computing'),(453,'health application'),(454,'NLP'),(455,'health applications'),(456,'paralinguistics'),(457,'learning'),(458,'depression'),(459,'deep learning'),(460,'lexical stress and pre-pausal lengthening'),(461,'lexical stress pre - pausal lengthening'),(462,'duration modeling'),(463,'phonetic features'),(464,'parkinson’s disease'),(465,'dysarthria'),(466,'word error rate'),(467,'variational autoencoder'),(468,'CMN2'),(469,'DAN'),(470,'VAE'),(471,'SRE18'),(472,'domain adaptation'),(473,'SRE16'),(474,'CMN'),(475,'VDANN'),(476,'DANN'),(477,'domain adversarial training'),(478,'lstm neural network'),(479,'continuous mandarin speech'),(480,'tone modeling and recognition'),(481,'target approximation model'),(482,'incremental speech synthesis'),(483,'representation learning'),(484,'representation'),(485,'MUSHRA'),(486,'SHR'),(487,'acoustic modelling'),(488,'training dynamics'),(489,'MFC'),(490,'average frequency response'),(491,'MFCC'),(492,'raw waveform'),(493,'multi - modal'),(494,'ASV'),(495,'spoofing; sub-band countermeasures; presentation attack detection; asvspoof; speaker verification'),(496,'speech evaluation'),(497,'speech feature extraction'),(498,'FVDM'),(499,'vowel distortion'),(500,'disordered speech'),(501,'huntington disease'),(502,'neural machine translation'),(503,'machine translation'),(504,'AVS'),(505,'multimodal learning'),(506,'multi-task learning'),(507,'TMT'),(508,'AVSD'),(509,'audio-visual scene-aware dialog'),(510,'MTN'),(511,'NMT'),(512,'code switching'),(513,'E2E'),(514,'multilingual'),(515,'code-switching'),(516,'multi-dialectal'),(517,'MSA'),(518,'conformer'),(519,'end end'),(520,'on-device'),(521,'limited memory'),(522,'device'),(523,'end-to-end'),(524,'image based jaw measurement'),(525,'jaw movement simulation'),(526,'articulatory simulation'),(527,'jaw kinematics'),(528,'simulation'),(529,'nasal'),(530,'velum'),(531,'articulatory phonology'),(532,'speech imaging'),(533,'phonology'),(534,'nasal consonants'),(535,'nasals'),(536,'dialogue learning'),(537,'reinforcement learning'),(538,'emotional text tospeech synthesis'),(539,'speech emotion recognition'),(540,'emotional text-tospeech synthesis'),(541,'ITU'),(542,'POLQA'),(543,'MOS'),(544,'VCTK'),(545,'transfer learning'),(546,'speech rate'),(547,'obstruents'),(548,'french german interference'),(549,'french/german interferences'),(550,'L2'),(551,'voicing assimilation'),(552,'language identification'),(553,'TCS'),(554,'end-to-end neural diarization'),(555,'SEAME'),(556,'CSM'),(557,'STC'),(558,'temporal attention'),(559,'end end neural diarization'),(560,'BLSTM'),(561,'EAM'),(562,'AME'),(563,'self-attention'),(564,'language diarization'),(565,'voice source'),(566,'laplace transform'),(567,'glottal airflow'),(568,'voice'),(569,'frequency domain lf model'),(570,'aliasing'),(571,'aliase'),(572,'phasor');
/*!40000 ALTER TABLE `ver0_keyword` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `ver0_paper`
--

DROP TABLE IF EXISTS `ver0_paper`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `ver0_paper` (
  `id` int NOT NULL AUTO_INCREMENT,
  `title` varchar(1024) NOT NULL,
  `abstract` longtext NOT NULL,
  `url` varchar(1024) NOT NULL,
  `conference_id` int NOT NULL,
  PRIMARY KEY (`id`),
  KEY `ver0_paper_conference_id_51742789_fk_ver0_conference_id` (`conference_id`),
  CONSTRAINT `ver0_paper_conference_id_51742789_fk_ver0_conference_id` FOREIGN KEY (`conference_id`) REFERENCES `ver0_conference` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=101 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `ver0_paper`
--

LOCK TABLES `ver0_paper` WRITE;
/*!40000 ALTER TABLE `ver0_paper` DISABLE KEYS */;
INSERT INTO `ver0_paper` VALUES (1,'Active appearance models for photorealistic visual speech synthesis','The perceived quality of a synthetic visual speech signal greatly depends on the smoothness of the presented visual articulators. This paper explains how concatenative visual speech synthesis systems can apply active appearance models to achieve a smooth and natural visual output speech. By modeling the visual speech contained in the system\'s speech database, a diversification between the synthesis of the shape and the texture of the talking head is feasible. This allows the system to accurately balance between the articulation strength of the visual articulators and the signal smoothness of the visual mode in order to optimize the synthesis. To improve the synthesis quality, an automatic database normalization strategy has been designed that removes variations from the database which are not related to speech production. As was verified by a perception experiment, this normalization strategy significantly improves the perceived signal quality.','https://www.isca-speech.org/archive/pdfs/interspeech_2010/mattheyses10_interspeech.pdf',1),(2,'Turn taking-based conversation detection by using DOA estimation','We propose a new method that detects conversation groups when multi-conversation groups exist simultaneously. The proposed method uses hands-free microphone arrays without wearable microphones. It has two main features: (a) We integrate a conventional turn taking-based conversation detection method with Direction of Arrival (DOA) estimation-based Voice Activity Detection (VAD). (b) The proposed method estimates the number of speakers for DOA estimation-based VAD by using turn taking rules. Experimental results indicate that the performance of the proposed method with only microphone arrays setup in rooms is comparable to that of the conventional methods with wearable microphones.','https://www.isca-speech.org/archive/pdfs/interspeech_2010/kawaguchi10_interspeech.pdf',1),(3,'Statistical multi-stream modeling of real-time MRI articulatory speech data','This paper investigates different statistical modeling frameworks for articulatory speech data obtained using real-time (RT) magnetic resonance imaging (MRI). To quantitatively capture the spatio-temporal shaping process of the human vocal tract during speech production a multi-dimensional stream of image features is derived from the MRI recordings. The features are closely related, though not identical, to the tract variables commonly defined in the articulatory phonology theory. The modeling of the shaping process aims at decomposing the articulatory data streams into primitives by segmentation, and the segmentation task is carried out using vector quantizers, Gaussian Mixture Models, Hidden Markov Models, and a coupled Hidden Markov Model. We evaluate the performance of the different segmentation schemes qualitatively with the help of a well understood data set which was used in a earlier study of inter-articulatory timing phenomena of American English nasal sounds.','https://www.isca-speech.org/archive/pdfs/interspeech_2010/bresch10_interspeech.pdf',1),(4,'Estimating noise from noisy speech features with a monte carlo variant of the expectation maximization algorithm','In this work, we derive a Monte Carlo expectation maximization algorithm for estimating noise from a noisy utterance. In contrast to earlier approaches, where the distribution of noise was estimated based on a vector Taylor series expansion, we use a combination of importance sampling and Parzen-window density estimation to numerically approximate the occurring integrals with the Monte Carlo method. Experimental results show that the proposed algorithm has superior convergence properties, compared to previous implementations of the EM algorithm. Its application to speech feature enhancement reduced the word error rate by over 30%, on a phone number recognition task recorded in a (real) noisy car environment.','https://www.isca-speech.org/archive/pdfs/interspeech_2010/faubel10_interspeech.pdf',1),(5,'Floor holder detection and end of speaker turn prediction in meetings','We propose a novel fully automatic framework to detect which meeting participant is currently holding the conversational floor and when the current speaker turn is going to finish. Two sets of experiments were conducted on a large collection of multiparty conversations: the AMI meeting corpus. Unsupervised speaker turn detection was performed by post-processing the speaker diarization and the speech activity detection outputs. A supervised end-of-speaker-turn prediction framework, based on Dynamic Bayesian Networks and automatically extracted multimodal features (related to prosody, overlapping speech, and visual motion), was also investigated. These novel approaches resulted in good floor holder detection rates (13.2% Floor Error Rate), attaining state of the art end-of-speaker-turn prediction performances.','https://www.isca-speech.org/archive/pdfs/interspeech_2010/dielmann10_interspeech.pdf',1),(6,'Automatic speech recognition of multiple accented English data','Accent variability is an important factor in speech that can significantly degrade automatic speech recognition performance. We investigate the effect of multiple accents on an English broadcast news recognition system. A multi-accented English corpus is used for the task, including broadcast news segments from 6 different geographic regions: US, Great Britain, Australia, North Africa, Middle East and India. There is significant performance degradation of a baseline system trained on only US data when confronted with shows from other regions. The results improve significantly when data from all the regions are included for accent-independent acoustic model training. Further improvements are achieved when MAP-adapted accent-dependent models are used in conjunction with a GMM accent classifier.','https://www.isca-speech.org/archive/pdfs/interspeech_2010/vergyri10_interspeech.pdf',1),(7,'Approaching human listener accuracy with modern speaker verification','Being able to recognize people from their voice is a natural ability that we take for granted. Recent advances have shown significant improvement in automatic speaker recognition performance. Besides being able to process large amount of data in a fraction of time required by human, automatic systems are now able to deal with diverse channel effects. The goal of this paper is to examine how state-of-the-art automatic system performs in comparison with human listeners, and to investigate the strategy for human-assisted form of automatic speaker recognition, which is useful in forensic investigation. We set up an experimental protocol using data from the NIST SRE 2008 core set. A total of 36 listeners have participated in the listening experiments from three sites, namely Australia, Finland and Singapore. State-of-the-art automatic system achieved 20% error rate, whereas fusion of human listeners achieved 22%.','https://www.isca-speech.org/archive/pdfs/interspeech_2010/hautamaki10_interspeech.pdf',1),(8,'Autoregressive clustering for HMM speech synthesis','The autoregressive HMM has been shown to provide efficient parameter estimation and high-quality synthesis, but in previous experiments decision trees derived from a non-autoregressive system were used. In this paper we investigate the use of autoregressive clustering for autoregressive HMM-based speech synthesis. We describe decision tree clustering for the autoregressive HMM and highlight differences to the standard clustering procedure. Subjective listening evaluation results suggest that autoregressive clustering improves the naturalness of the resulting speech. We find that the standard minimum description length (MDL) criterion for selecting model complexity is inappropriate for the autoregressive HMM. Investigating the effect of model complexity on naturalness, we find that a large degree of overfitting is tolerated without a substantial decrease in naturalness.','https://www.isca-speech.org/archive/pdfs/interspeech_2010/shannon10_interspeech.pdf',1),(9,'Exploiting variety-dependent phones in portuguese variety identification applied to broadcast news transcription','This paper presents a Variety IDentification (VID) approach and its application to broadcast news transcription for Portuguese. The phonotactic VID system, based on Phone Recognition and Language Modelling, focuses on a single tokenizer that combines distinctive knowledge about differences between the target varieties. This knowledge is introduced into a Multi-Layer Perceptron phone recognizer by training mono-phone models for two varieties as contrasting phone-like classes. Significant improvements in terms of identification rate were achieved compared to conventional single and fused phonotactic and acoustic systems. The VID system is used to select data to automatically train variety-specific acoustic models for broadcast news transcription. The impact of the selection is analyzed and variety-specific recognition is shown to improve results by up to 13% compared to a standard variety baseline.','https://www.isca-speech.org/archive/pdfs/interspeech_2010/koller10_interspeech.pdf',1),(10,'A pointwise approach to pronunciation estimation for a TTS front-end','In this paper, we propose a pointwise approach to the Japanese TTS front-end. In this approach, phoneme sequence estimation of sentences is decomposed into two tasks: word segmentation of the input sentence and phoneme estimation of each word. Then these two tasks are solved by pointwise classifiers without referring to the neighboring classification results. In contrast to existing sequence-based methods, an n-gram model based on sequences of word-phoneme pairs for example, this framework enables us to use various language resources such as sentences in which only a few words are annotated, or an unsegmented list of compound words, among others. In the experiments, we compared a joint tri-gram model with the combination of a pointwise word segmenter and a pointwise phoneme sequence estimator. The results showed that our framework successfully enables a TTS front-end to refer to a partially annotated corpus and/or a word sequence list annotated with phoneme sequences to realize a far larger improvement in accuracy.','https://www.isca-speech.org/archive/pdfs/interspeech_2011/mori11_interspeech.pdf',2),(11,'Spoken document confidence estimation using contextual coherence','Selecting well-recognized transcripts is critical if information retrieval systems are to extract business intelligence from massive spoken document databases. To achieve this goal, we target spoken document confidence measures that represent the recognition rates of each document. We focus on the incoherent word occurrences over several utterances in ill-recognized transcripts of spoken documents. The proposed method uses contextual coherence as a measure of spoken document confidence. The contextual coherence is formulated as the mean of pointwise mutual information (PMI). We also propose a smoothing method of PMI, which deals with the data sparseness problem. Compared to the conventional method, our smoothing technique offers improved correlation coefficients between spoken document confidence scores and recognition rates from 0.573 to 0.672. Moreover, an even higher correlation coefficient, 0.710, is achieved by combining the contextual-based and decoder-based confidence measures.','https://www.isca-speech.org/archive/pdfs/interspeech_2011/asami11_interspeech.pdf',2),(12,'Maximum likelihood i-vector space using PCA for speaker verification','This paper proposes a new approach to training the i-vector space using a variant of PCA with the Baum-Welch statistics for speaker verification. In eigenvoice the rank of variability space is bounded by the number of training speakers, so a variant of the probabilistic PCA approach is introduced for estimating the parameters. But this constraint doesn\'t exist in i-vector model because the number of utterances is much bigger than the rank of total variability space. We adopt the EM algorithm for PCA with the statistics to train the total variability space, and the maximum likelihood criterion is used. After WCCN, the cosine similarity scoring is used for decision. These two total variability spaces will be fused at feature-level and score-level. The experiments have been run on the NIST SRE 2008 data, and the results show that the performances in two total variability spaces are comparable. The performance can be improved obviously after feature fusion and score fusion.','https://www.isca-speech.org/archive/pdfs/interspeech_2011/lei11_interspeech.pdf',2),(13,'A rapid adaptation algorithm for tracking highly non-stationary noises based on Bayesian inference for on-line spectral change point detection','This paper presents an innovative rapid adaptation technique for tracking highly non-stationary acoustic noises. The novelty of this technique is that it can detect the acoustic change points from the spectral characteristics of the observed speech signal in rapidly changing non-stationary acoustic environments. The proposed innovative noise tracking technique will be very suitable for joint additive and channel distortions compensation (JAC) for on-line automatic speech recognition (ASR). The Bayesian on-line change point detection (BOCPD) approach is used to implement this technique. The proposed algorithm is tested using highly non-stationary noisy speech samples from the Aurora2 speech database. Significant improvement in minimizing the delay in adaptation to new acoustic conditions is obtained for highly nonstationary noises compared to the most popular baseline noise tracking algorithm MCRA and its derivatives.','https://www.isca-speech.org/archive/pdfs/interspeech_2011/chowdhury11_interspeech.pdf',2),(14,'Phrasal prominences do not need pitch movements: postfocal phrasal heads in Italian','Informationally Given phrases following an instance of focus are generally realized in a compressed pitch range and are generally assumed to lack prosodic prominences. In this paper, we address the question of the metrical representation of postfocal constituents in Tuscan Italian. The results of a production experiment show that, despite their being realized with a low and flat F0 contour, postfocal constituents are not extrametrical, but are phrased and assigned phrasal metrical prominences of phrasal level. The impact of our results on the prosodic representation of Italian and on the information structure-prosody interface are discussed.','https://www.isca-speech.org/archive/pdfs/interspeech_2011/bocci11_interspeech.pdf',2),(15,'Validating a second language perception model for classroom context - a longitudinal study within the perceptual assimilation model','The present study verified whether adult listeners retain the ability to improve non-native speech perception and if it can be significantly enhanced in the formal context, a very impoverished context with respect to the natural one. We tested (i) whether perceptual learning is possible for adults in a classroom context during focused phonetic lessons, and (ii) whether it follows the pattern predicted for natural acquisition by the PAM-L2 [1]. The results showed that adult listeners are still able to improve foreign sound perception and this ability seems to occur also in formal contexts in line with the PAM-L2 predictions.','https://www.isca-speech.org/archive/pdfs/interspeech_2011/sisinni11_interspeech.pdf',2),(16,'Phone impact based speech transmission technique for reliable speech recognition in poor wireless network conditions','This paper presents a preliminary study on an effective differentiable network service technique to achieve improved speech recognition under severely poor wireless channel conditions, by leveraging multiple priority levels applied to speech classes. Each speech class is assigned a different priority level based on its level of impact on speech recognition performance. Based on their priority level, frames of each speech class are given distinct levels of network quality of service (QoS) to satisfy the delay requirement and enable speech recognition at the receiver. This proposed Phone Impact (PI) based priority class is compared to the Voiced/Unvoiced (VU) based priority class in this study. The experimental results prove that the proposed scheme is effective at providing wireless network service for robust speech recognition under poor channel conditions, showing up to 2.67 dB and 5.93 dB lower Signal to Noise Ratio (SNR) operating regions compared to the VU based and plain protocols respectively. The PI based method also shows acceptable WERs at lower SNRs where VU and plain systems significantly degrade in speech recognition performance in case of retry limit of 6.','https://www.isca-speech.org/archive/pdfs/interspeech_2011/taufique11_interspeech.pdf',2),(17,'Evaluation of i-vector speaker recognition systems for forensic application','This paper contributes a study on i-vector based speaker recognition systems and their application to forensics. The sensitivity of i-vector based speaker recognition is analyzed with respect to the effects of speech duration. This approach is motivated by the potentially limited speech available in a recording for a forensic case. In this context, the classification performance and calibration costs of the i-vector system are analyzed along with the role of normalization in the cosine kernel. Evaluated on the NIST SRE-2010 dataset, results highlight that normalization of the cosine kernel provided improved performance across all speech durations compared to the use of an unnormalized kernel. The normalized kernel was also found to play an important role in reducing miscalibration costs and providing well-calibrated likelihood ratios with limited speech duration.','https://www.isca-speech.org/archive/pdfs/interspeech_2011/mandasari11_interspeech.pdf',2),(18,'Real-time visualization of English pronunciation on an IPA chart based on articulatory feature extraction','In recent years, Computer Assisted Pronunciation Technology (CAPT) systems have been developed that can help Japanese learners to study foreign languages. We have been developing a pronunciation training system to evaluate and correct learner\'s pronunciation by extracting articulatory-features (AFs). In this paper, we propose a novel pronunciation training system that can plot the place and manner of articulation of learner\'s pronunciation on an International Phonetic Alphabet (IPA) chart in real time. First, the proposed system converts input speech into AF-sequences by using multi-layer neural networks (MLNs). Then, the AF-sequences are converted into x-y coordinates and plotted on an IPA chart to show his/her articulation in real time. Lastly, we investigate plotting accuracies on the IPA chart through experimental evaluation.','https://www.isca-speech.org/archive/pdfs/interspeech_2012/iribe12_interspeech.pdf',3),(19,'Enhancing vocal tract length normalization with elastic registration for automatic speech recognition','Vocal tract length normalization (VTLN) is commonly applied utterance-wise with a warping function which makes the assumption of a linear dependence between the vocal tract length and the location of the formants. In this work we propose a data-driven method for enhancing the performance of systems that already use standard VTLN. The method is based on elastic registration to estimate optimal nonparametric transformations to further reduce inter-speaker variabilities. Results show that the proposed method can increase the performance of monophone systems such that it reaches that of a triphone system.','https://www.isca-speech.org/archive/pdfs/interspeech_2012/muller12_interspeech.pdf',3),(20,'Overlapped speech detection in meeting using cross-channel spectral subtraction and spectrum similarity','We propose an overlapped speech detection method for speech recognition and speaker diarization of meetings, where each speaker wears a lapel microphone. Two novel features are utilized as inputs for a GMM-based detector. One is speech power after cross-channel spectral subtraction which reduces the power from the other speakers. The other is an amplitude spectral cosine correlation coefficient which effectively extracts the correlation of spectral components in a rather quiet condition. We evaluated our method using a meeting speech corpus of four persons. The accuracy of our proposed method, 74.1%, was significantly better than that of the conventional method, 67.0%, which uses raw speech power and power spectral Pearson\'s correlation coefficient.','https://www.isca-speech.org/archive/pdfs/interspeech_2012/yokoyama12_interspeech.pdf',3),(21,'Hidden Markov convolutive mixture model for pitch contour analysis of speech','This paper proposes a statistical model of speech F0 contours, which is based on the discrete-time version of the Fujisaki model. Our motivation for formulating this model is incorporating F0 contours into various statistical speech processing problems. In this paper, we describe the formulation of the model and quantitatively evaluates the performance of the model through Fujisaki-model parameter estimations from real speech F0 contours. Compared with another speech F0 model we have proposed, the present model prefer fitting observed F0 contours because the previous model is based on a squared error criterion in the Fujisaki-model commands domain and the present model is in the F0 contours domain.','https://www.isca-speech.org/archive/pdfs/interspeech_2012/yoshizato12_interspeech.pdf',3),(22,'Vowels produced by sliding three-tube model with different lengths','The sliding three-tube (S3T) model, based on Fant\'s acoustic theory and proposed in our previous studies, has a simple structure, enabling it to produce human-like vowels useful for education in acoustics and speech science. In this study, we changed the size of the S3T model and combined it with sound sources with different fundamental frequencies. We confirmed that the models could produce vowels of different speaker types. We were able to retain good vowel quality for a perceptual study when we simultaneously shortened vocal-tract length and increased fundamental frequency. We also discussed the models in a new way, comparing children\'s and adults\' vowels, especially for educational purposes.','https://www.isca-speech.org/archive/pdfs/interspeech_2012/arai12_interspeech.pdf',3),(23,'A robust unsupervised arousal rating framework using prosody with cross-corpora evaluation','This paper presents an unsupervised method for producing a bounded rating of affective arousal from speech. One of the major challenges in such behavioral signal classification is the design of methods that generalize well across domains and datasets. We propose a framework that provides robustness across databases by: selecting coherent features based on empirical and theoretical evidence, fusing activation confidences from multiple features, and effectively weighting the soft-labels without knowing the true labels. Spearman\'s rank-correlation (and binary classification accuracy) on four arousal databases are: 0.62 (73%), 0.77 (86%), 0.70 (82%), and 0.65 (73%).','https://www.isca-speech.org/archive/pdfs/interspeech_2012/bone12_interspeech.pdf',3),(24,'Model-based approaches to adaptive training in reverberant environments','Adaptive training is a powerful approach for building speech recognition systems using non-homogeneous data. This work presents an extension of model-based adaptive training to handle reverberant environments. The recently proposed Reverberant VTS-Joint (RVTSJ) adaptation is used to factor out unwanted additive and reverberant noise variations in multiconditional training data, yielding a canonical model neutral to noise conditions. An maximum likelihood estimation of the canonical model parameters is described. An initialisation scheme that uses the VTS-based adaptive training to initialise the model parameters is also presented. Experiments are conducted on a reverberant simulated AURORA4 task.','https://www.isca-speech.org/archive/pdfs/interspeech_2012/wang12g_interspeech.pdf',3),(25,'The influence of F0 contour continuity on prominence perception','The presented study concerns the influence of the syllabic structure on perceived prominence. We examined how gaps in the F0 contour due to unvoiced consonants affect prominence perception, given that such gaps can either be filled or blinded out by listeners. For this purpose we created a stimulus set of real disyllabic words which differed in the quantity of the vowel of the accented syllable nucleus and the types of subsequent intervocalic consonant(s). Results include, inter alia, that stimuli with unvoiced gaps in the F0 contour are indeed perceived as less prominent. The prominence reduction is smaller for monotonous stimuli than for stimuli with F0 excursions across the accented syllable. Moreover, in combination with F0 excursions, it also mattered whether F0 had to be interpolated or extrapolated, and whether or not the gap included a fricative sound. The results support both the filling-in and blinding-out of F0 gaps, which fits in well with earlier experiments on the production and perception of pitch.','https://www.isca-speech.org/archive/pdfs/interspeech_2013/mixdorff13_interspeech.pdf',4),(26,'The edinburgh speech production facility doubletalk corpus','The DoubleTalk articulatory corpus was collected at the Edinburgh Speech Production Facility (ESPF) using two synchronized Carstens AG500 electromagnetic articulometers. The first release of the corpus comprises orthographic transcriptions aligned at phrasal level to EMA and audio data for each of 6 mixed-dialect speaker pairs. It is available from the ESPF online archive. A variety of tasks were used to elicit a wide range of speech styles, including monologue (a modified Comma Gets a Cure and spontaneous story-telling), structured spontaneous dialogue (Map Task and Diapix), a wordlist task, a memory-recall task, and a shadowing task. In this session we will demo the corpus with various examples.','https://www.isca-speech.org/archive/pdfs/interspeech_2013/scobbie13_interspeech.pdf',4),(27,'Vowel and prosodic factor dependent variations of vocal-tract length','We have measured total vocal-tract (VT) length, lip-tube length and glottal height during vowels on X-ray film data of short French utterances. VT midpoints are determined by progressively fitting circles along the VT-length from the glottis to lip opening. The VT-length is obtained by summing up the distance between adjacent midpoints. Lip-tube length is defined as the distance between the incisors and the lip opening along the midline. Results show that the range of VT-length variation is 3.2cm with the average VTlength of 16.4cm. The cause of this large range appears to be the combination of the vowel dependent VT-length and prosodic position that influences on the glottal height. For example, during a vowel at sentence final position, glottis goes down with falling intonation or up with rising intonation corresponding to, respectively, VT lengthening or shortening. The lip-tube lengths are little affected by prosodic position and exhibit a clear vowel dependency. The prosodic influences manifested on the glottal height are not compensated but rather expanded in the VT-length, yet maintaining the characteristic vowel-dependency. This suggests an underlying mechanism to maintain a uniform stretching/compression of VTlength, which tends to hold the phonetic identity of a vowel under large VT-length changes.','https://www.isca-speech.org/archive/pdfs/interspeech_2013/maeda13_interspeech.pdf',4),(28,'Identifying consonantal tasks via measures of tongue shaping: a real-time MRI investigation of the production of vocalized syllabic /l/ in American English','Liquids are unique in their ability to occupy syllable onset, nucleus, and coda positions in American English, as well as the fact that they are composed of two lingual gestures. Upon inspection of /l/ in syllable nucleus and coda positions using real-time MRI, it appears that the tongue tip constriction we might expect for /l/ is often not present, a phenomenon called /l/-vocalization. However, it is not merely the case that the consonantal gesture of /l/ is completely lost in these syllable positions, leaving behind a simple vocalic configuration. Though there is often no raising of the tongue tip in an attempt to make contact with the alveolar ridge, the /l/ exhibits complex tongue shaping involving curling in the region of the tongue blade. The result is a lowered tongue blade relative to the tongue tip and dorsum. This shaping is captured through measures of Gaussian curvature at evenly spaced points along the tongue. The results indicate that /l/-vocalization in the syllable rhyme does not involve a complete loss of the consonantal nature of the lateral, but rather a modification of its realization.','https://www.isca-speech.org/archive/pdfs/interspeech_2013/smith13c_interspeech.pdf',4),(29,'The organ stop “vox humana” as a model for a vowel synthesiser','In mechanical speech synthesis reed pipes were mainly used for the generation of the voice. The organ stop \"vox humana\" played a central role for this concept. Historical documents report that the \"vox humana\" sounded like human vowels. In this study tones of four different \"voces humanae\" were recorded to investigate the similarity to human vowels. The acoustical and perceptual analysis revealed that some though not all tones show a high similarity to selected vowels.','https://www.isca-speech.org/archive/pdfs/interspeech_2013/brackhane13_interspeech.pdf',4),(30,'Artificial bandwidth extension based on regularized piecewise linear mapping with discriminative region weighting and long-Span features','Artificial Bandwidth Extension (ABE) has been introduced to improve perceived speech quality and intelligibility of narrow-band telephone speech. Most of the existing algorithms divided ABE into 2 sub-problems, namely extension of the excitation signal and that of the spectral envelope. In this paper, we propose a new method for spectral envelope extension based on REgularized piecewise linear mapping with DIscriminative region weighting And Longspan features (REDIAL). REDIAL is a revised version of SPLICE, a well-known method for speech enhancement. In REDIAL, however, discriminative model is introduced for space division step of the original SPLICE. The proposed REDIAL-based method approximates non-linear transformation from narrowband features to their wideband counterpart by a summation of piecewise linear transformations. The proposed method was compared with the widely used GMM-based method, through objective and subjective evaluations in both speaker-dependent and speaker-independent conditions. Both evaluations showed that the proposed method significantly outperforms the conventional GMM-based method.','https://www.isca-speech.org/archive/pdfs/interspeech_2013/duy13_interspeech.pdf',4),(31,'An investigation of vowel epenthesis in Chinese learners\' production of German consonants','The present study investigates the influence of phonetic factors on the frequency of vowel epenthesis in the German speech of Chinese learners. The subjects were intermediate learners of German who entered Germany within five months of their study. Descriptive statistics were performed on the data collected from reading tasks, and phonetic analysis was provided to explain the phenomenon of epenthesis. In the main experiment, eighteen Chinese students were recruited to read 50 phonetically rich sentences with various sentence modes after one month residence in Germany. Results indicate that these learners employed the epenthesis strategy more or less in producing consonant codas and consonant onset clusters in German. An investigation in the frequency of epenthesis in relation to various factors demonstrates that consonant cluster length, L1 transfer, markedness, sonority, and articulatory timing influence the occurrences of epenthesis simultaneously. An additional experiment was conducted after a time span of three months, ten of these subjects were requested to read the same text, the result shows that the amount of epenthesis decreases with the increase of the length of residence and German language learning experience. These findings might shed some light on the acquisition process of consonant codas in foreign languages.','https://www.isca-speech.org/archive/pdfs/interspeech_2013/ding13_interspeech.pdf',4),(32,'Text-to-speech inspired duration modeling for improved whole-word acoustic models','In the construction of whole-word acoustic models, we have previously demonstrated substantial gains by using MAP estimation to introduce a simple prior model of phonetic timing. Based solely on the word\'s phonetic (dictionary) pronunciation, this simple model included no information about the individual durations of constituent phones. However, the problem of modeling segmental duration has long been studied in the text-to-speech (TTS) community. We draw upon this work to develop a classification and regression tree (CART) approach for constructing prior models of phonetic timing which considers factors such as syllable stress, syllable position, adjacent phone class and voicing. This improved prior model closes 33% of the gap in keyword spotting performance between highly supervised whole-word models and those estimated without any examples.','https://www.isca-speech.org/archive/pdfs/interspeech_2013/kintzley13_interspeech.pdf',4),(33,'Distributed asynchronous optimization of convolutional neural networks','Recently, deep Convolutional Neural Networks have been shown to outperform Deep Neural Networks for acoustic modelling, producing state-of-the-art accuracy in speech recognition tasks. Convolutional models provide increased model robustness through the usage of pooling invariance and weight sharing across spectrum and time. However, training convolutional models is a very computationally expensive optimization procedure, especially when combined with large training corpora. In this paper, we present a novel algorithm for scalable training of deep Convolutional Neural Networks across multiple GPUs. Our distributed asynchronous stochastic gradient descent algorithm incorporates sparse gradients, momentum and gradient decay to accelerate the training of these networks. Our approach is stable, neither requiring warm-starting or excessively large minibatches. Our proposed approach enables convolutional models to be efficiently trained across multiple GPUs, enabling a model to be scaled asynchronously across 5 GPU workers with ˜68% efficiency.','https://www.isca-speech.org/archive/pdfs/interspeech_2014/chan14_interspeech.pdf',5),(34,'Combination of multilingual and semi-supervised training for under-resourced languages','Multilingual training of neural networks for ASR is widely studied these days. It has been shown that languages with little training data can benefit largely from the multilingual resources for training. The use of unlabeled data for the neural network training in semi-supervised manner has also improved the ASR system performance. Here, the combination of both methods is presented. First, multilingual training is performed to obtain an ASR system to automatically transcribe the unlabeled data. Then, the automatically transcribed data are added. Two neural networks are trained — one from random initialization and one adapted from multilingual network — to evaluate the effect of multilingual training under presence of larger amount of training data. Further, the CMLLR transform is applied in the middle of the stacked Bottle-Neck neural network structure. As the CMLLR rotates the features to better fit given model, we evaluated whether it is better to adapt the existing NN on the CMLLR features or if it is better to train it from random initialization. The last step in our training procedure is the fine-tuning on the original data.','https://www.isca-speech.org/archive/pdfs/interspeech_2014/grezl14_interspeech.pdf',5),(35,'On the selection of the impulse responses for distant-speech recognition based on contaminated speech training','Distant-speech recognition represents a technology of fundamental importance for future development of assistive applications characterized by flexible and unobtrusive interaction in home environments. State-of-the-art speech recognition still exhibits lack of robustness, and an unacceptable performance variability, due to environmental noise, reverberation effects, and speaker position. In the past, multi-condition training and contamination methods were explored to reduce the mismatch between training and test conditions. However, the performance evaluation can be biased by factors as limited number of positions of speaker and microphones, adopted set of impulse responses, vocabulary and grammars defining the recognition task. The purpose of this paper is to investigate in more detail some critical aspects that characterize such experimental context. To this purpose, our work addressed a microphone network distributed over different rooms of an apartment and a related set of speaker-microphone pairs leading to a very large set of impulse responses. Besides simulations, the experiments also tackled real speech interactions. The performance evaluation was based on a phone-loop task, in order to minimize the influence of linguistic constraints. The experimental results show how less critical is an accurate selection of impulse responses, if compared to other factors as the signal-to-noise ratio introduced by additive background noise.','https://www.isca-speech.org/archive/pdfs/interspeech_2014/ravanelli14_interspeech.pdf',5),(36,'Intrinsic spectral analysis based on temporal context features for query-by-example spoken term detection','We investigate the use of intrinsic spectral analysis (ISA) for query-by-example spoken term detection (QbE-STD). In the task, spoken queries and test utterances in an audio archive are converted to ISA features, and dynamic time warping is applied to match the feature sequence in each query with those in test utterances. Motivated by manifold learning, ISA has been proposed to recover from untranscribed utterances a set of nonlinear basis functions for the speech manifold, and shown with improved phonetic separability and inherent speaker independence. Due to the coarticulation phenomenon in speech, we propose to use temporal context information to obtain the ISA features. Gaussian posteriorgram, as an efficient acoustic representation usually used in QbE-STD, is considered a baseline feature. Experimental results on the TIMIT speech corpus show that the ISA features can provide a relative 13.5% improvement in mean average precision over the baseline features, when the temporal context information is used.','https://www.isca-speech.org/archive/pdfs/interspeech_2014/yang14c_interspeech.pdf',5),(37,'Crowdee: mobile crowdsourcing micro-task platform for celebrating the diversity of languages','This paper introduces a novel crowdsourcing platform provided to the community. The platform operates on mobile devices and makes data generation and labeling scenarios available for many related research tracks potentially covering also small and underrepresented languages. Besides the versatile ways for commencing studies using the platform, also active research on crowdsourcing itself becomes feasible. With special focus on speech- and video recordings, the mobility and scalability of the platform is expected to stimulate and foster data-driven studies and insights throughout the community.','https://www.isca-speech.org/archive/pdfs/interspeech_2014/naderi14_interspeech.pdf',5),(38,'Spectral tilt modelling with GMMs for intelligibility enhancement of narrowband telephone speech','In mobile communications, post-processing methods are used to improve the intelligibility of speech in adverse background noise conditions. In this study, post-processing based on modelling the Lombard effect is investigated. The study focuses on comparing different spectral envelope estimation methods together with Gaussian mixture modelling in order to change the spectral tilt of speech in a post-processing algorithm. Six spectral envelope estimation methods are compared using objective distortion measures as well as subjective word-error rate and quality tests in different near-end noise conditions. Results show that one of the envelope estimation methods, stabilised weighted linear prediction, yielded statistically significant improvement in intelligibility over unprocessed speech.','https://www.isca-speech.org/archive/pdfs/interspeech_2014/jokinen14b_interspeech.pdf',5),(39,'A low complexity model adaptation approach involving sparse coding over multiple dictionaries','The work presented in this paper describes a novel on-line adaptation approach for extremely low adaptation data scenario. The proposed approach extends a similar redundant dictionary based approach reported recently in literature. In this work, the orthogonal matching pursuit (OMP) algorithm is used for bases selection instead of the matching pursuit (MP). This helps in avoiding the selection of an atom more than once. Furthermore, this work also explores the use of cluster-specific eigenvoices to capture local acoustic details unlike the conventional eigenvoices technique. These approaches are then combined to reduce the number of weight parameters being estimated for deriving adapted model. Towards this purpose, separate sparse coding of the test data is performed over a set of dictionaries. Those sparse coded supervectors are then scaled and used as the Gaussian mean parameter in the adapted model. Consequently, only a few scaling factors are needed to be estimated. Such a reduction in number of parameters is highly desirable for on-line applications where the latency is a major factor.','https://www.isca-speech.org/archive/pdfs/interspeech_2014/shahnawazuddin14_interspeech.pdf',5),(40,'English consonant confusions by Greek listeners in quiet and noise and the role of phonological short-term memory','This study investigated English consonant identification by Greek listeners and the role of phonological short-term memory (PSTM) in listeners\' identification ability. Twenty Greek university students who had received formal instruction in English identified 24 English consonants (embedded in VCV syllables) presented in quiet and in two noise types, a competing talker at a signal-to-noise ratio (SNR) of -6dB and an 8-speaker babble at an SNR of -2dB. Participants\' PSTM was assessed via a serial non-word recognition task in Greek. The results showed that identification scores in quiet were significantly higher than in noise. There was no difference in scores between the two noise conditions. PSTM correlated with English consonant identification in quiet and in the two types of noise; listeners with greater PSTM capacity were also better in identifying English consonants in quiet and noise, a finding that extends previous research in quiet to L2 perception in adverse listening conditions. English consonant confusion patterns are interpreted as caused by a combination of first-language interference (at both the phonetic and phonological levels) and spectral/articulatory factors.','https://www.isca-speech.org/archive/pdfs/interspeech_2014/lengeris14_interspeech.pdf',5),(41,'Unsupervised training methods for discriminative language modeling','Discriminative language modeling (DLM) aims to choose the most accurate word sequence by reranking the alternatives output by the automatic speech recognizer (ASR). The conventional (supervised) way of training a DLM requires a large amount of acoustic recordings together with their manual reference transcriptions. These transcriptions are used to determine the target ranks of the ASR outputs, but may be hard to obtain. Previous studies make use of the existing transcribed data to build a confusion model which boosts the training set by generating artificial data: a process known as semi-supervised training. In this study we concentrate on the unsupervised setting where no manual transcriptions are available at all. We propose three ways to determine a sequence that could serve as the missing reference text and two approaches which use this information to (i) determine the ranks of the ASR outputs in order to train the discriminative model directly, and (ii) build a confusion model in order to generate artificial training examples. We compare our techniques with the supervised and the semi-supervised setups. Using the reranking variant of the WER-sensitive perceptron algorithm, we obtain word error rate improvements up to half of those of the supervised case.','https://www.isca-speech.org/archive/pdfs/interspeech_2014/dikici14_interspeech.pdf',5),(42,'Effect of spectral degradation to the intelligibility of vowel sentences','Based on the noise-replacement paradigm, recent studies showed that vowels carried more perceptional information for sentence intelligibility than consonants. Considering that vowels contain many important acoustic cues for speech perception, this study further assessed the effect of spectral degradation to the intelligibility of Mandarin vowel sentences. Mandarin sentences were processed to generate three types of spectrally degraded [i.e., fundamental frequency (F0) flattened, sinewave synthesized, and noise-vocoded] stimuli. Noise-replacement paradigm was implemented to preserve different amounts of vowel centers and replace the rest with noise. Listening experiments showed that flattening F0 had a minimal effect on the intelligibility of Mandarin vowel sentences, and the harmonic structure within vowels accounted more for the intelligibility of Mandarin vowel sentences. While deleting vowel edges had little influence on the intelligibility of the unprocessed vowel sentences, it had a significantly negative effect on the intelligibility of vowel sentences with spectral degradation.','https://www.isca-speech.org/archive/pdfs/interspeech_2014/chen14i_interspeech.pdf',5),(43,'Auto-imputing radial basis functions for neural-network turn-taking models','A stochastic turn-taking (STT) model is a per-frame predictor of incipient speech activity. Its ability to make predictions at any instant in time makes it particularly well-suited to the analysis and synthesis of interactive conversation. At the current time, however, STT models are limited by their inability to accept features which may frequently be undefined. Rather than attempting to impute such features, this work proposes and evaluates a mechanism which implicitly conditions Gaussian-distributed features on Bernoulli-distributed indicator features, making prior imputation unnecessary. Experiments indicate that the proposed mechanisms achieve predictive parity with standard model structures, while at the same time offering more direct interpretability and the desired insensitivity to missing feature values.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/laskowski15_interspeech.pdf',6),(44,'Cross-modality matching of linguistic and emotional prosody','Talkers can express different meanings or emotions without changing what is said by changing how it is said (by using both auditory and/or visual speech cues). Typically, cue strength differs between the auditory and visual channels: linguistic prosody (expression) is clearest in audition; emotional prosody is clearest visually. We investigated how well perceivers can match auditory and visual linguistic and emotional prosodic signals. Previous research showed that perceivers can match linguistic visual and auditory prosody reasonably well. The current study extended this by also testing how well auditory and visual spoken emotion expressions could be matched. Participants were presented a pair of sentences (consisting of the same segmental content) spoken by the same talker and were required to decide whether the pair had the same prosody. Twenty sentences were tested with two types of prosody (emotional vs. linguistic), two talkers, and four matching conditions: auditory-auditory (AA); visual-visual (VV); auditory-visual (AV); and visual-auditory (VA). Linguistic prosody was accurately matched in all conditions. Matching emotional expressions was excellent for VV, poorer for VA, and near chance for AA and AV presentations. These differences are discussed in terms of the relationship between types of auditory and visual cues and task effects.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/simonetti15_interspeech.pdf',6),(45,'On evaluation metrics for social signal detection','Social signal detection is a task in speech technology which has recently became more popular. In the Interspeech 2013 ComParE Challenge one of the tasks was social signal detection, and since then, new results have been published on the dataset. These studies all used the Area Under Curve (AUC) metric to evaluate the performance; here we argue that this metric is not really suitable for social signals detection. Besides raising some serious theoretical objections, we will also demonstrate this unsuitability experimentally: we will show that applying a very simple smoothing function on the output of the frame-level scores of state-of-the-art classifiers can significantly improve the AUC scores, but perform poorly when employed in a Hidden Markov Model. As the latter is more like real-world applications, we suggest relying on utterance-level evaluation metrics in the future.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/gosztolya15b_interspeech.pdf',6),(46,'Perception of an existing and non-existing L2 English phoneme behind noise by Japanese native speakers','This study investigates how similarly a person hears an existing and non-existing speech sound behind noise in L2, as compared to L1 reported in Mattys, Barkan, and Samuel (2014). Participants were Japanese native speakers who spoke English as a second language. They listened to English words and non-words in which a phoneme was covered by noise (added) or replaced by noise (replaced). The target phoneme was either a liquid or a nasal. In experiment, participants listened to a pair of a word with noise (added or replaced) and a word without noise (normal) in a row, and evaluated the similarity of the two by using an 8-point scale. The results suggested that L2 listeners perceived the added and replaced sound significantly differently. L2 listeners found the added sound (a phoneme + noise) more similar to a normal sound than the replaced sound (noise only), as was also reported in L1 listeners. At the same time, they also perceived the illusory sound of a missing phoneme in the replaced condition. A missing nasal was significantly more restored than a missing liquid. There was no lexical effect in perceptual restoration of phonemes among L2 listeners, although it was reported among L1 listeners.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/ishida15_interspeech.pdf',6),(47,'Phonetic/linguistic web services at BAS','We present recent developments in the collection of phonetic-linguistic web services provided by the Bavarian Archive of Speech Signals (BAS). The BAS back end web services are REST based and can be easily integrated into user applications. Several public web interfaces have been implemented that utilize these back end services to provide easy-to-use access to high-end linguistic and phonetic processing (front end services). In this show&tell we demonstrate the latest front end services of BAS: automatic phonetic segmentation & labelling using the MAUS technique (14 languages), text-to-phoneme conversion (13 languages), automatic phonetic transcription (6 languages), phonetic syllabification (13 languages), and speech synthesis.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/kisler15_interspeech.pdf',6),(48,'The technology powering personal digital assistants','We have long envisioned that one day computers will understand natural language and anticipate what we need and when we need it to proactively complete tasks on our behalf. As computers get smaller and more pervasive, how humans interact with them is becoming a crucial issue. Despite numerous attempts over the past 40 years to make language understanding an effective and robust natural user interface for computer interaction, success was limited and scoped to applications that are not particularly central to everyday use. However, advances in speech recognition and machine learning, coupled with the emergence of structured data served by content providers and increased computational power have broadened the application of natural language understanding to a wide spectrum of everyday tasks that are central to the user\'s productivity. We believe that as computers become smaller and more ubiquitous (eg wearable computers) and as the number of applications increases, both system-initiated and user initiated task completion across various applications and services will become indispensable for personal life management and work productivity. There has been already a tremendous investment in the industry (particularly Microsoft, Google, Apple, Amazon and Nuance) around digital personal assistants during the last couple of years. Each of the major companies in the speech and language technology space has a version of their personal assistants (Cortana, Google Now, Siri, Echo, and Dragon, respectively) deployed in production. Yet there is not much talked about these technologies and products in any of the speech and language technology conferences. In this talk, we give an overview of personal digital assistants, describe the system design, architecture and the key components behind them. We will highlight challenges and describe best practices related to the bringing personal assistants from laboratories to the real-world and discuss their potential to fully redefine the human-computer interaction moving forward.','https://www.isca-speech.org/archive/interspeech_2015/sarikaya15_interspeech.html',6),(49,'Effect of trapping questions on the reliability of speech quality judgments in a crowdsourcing paradigm','This paper reports on a crowdsourcing study investigating the influence of trapping questions on the reliability of the collected data. The crowd workers were asked to provide quality ratings for speech samples from a standard database. In addition, they were presented with different types of trapping questions, which were designed based on previous research. The ratings obtained from the crowd workers were compared to ratings collected in a laboratory setting. Best results (i.e. highest correlation with and lowest root-mean-square deviation from the lab ratings) were observed for the type of trapping question, for which a recorded voice was presented in the middle of a random stimuli. The voice explained to the workers that high quality responses are important to us, and asked them to select a specific item to show their concentration. We hypothesize that this kind of trapping question communicates the importance and the value of their work to the crowd workers. Based on Herzberg two-factor theory of job satisfaction, the presence of factors, such as acknowledgment and the feeling of being valued, facilitates satisfaction and motivation, and eventually leads to better performance.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/naderi15_interspeech.pdf',6),(50,'Rapid adaptation for deep neural networks through multi-task learning','We propose a novel approach to addressing the adaptation effectiveness issue in parameter adaptation for deep neural network (DNN) based acoustic models for automatic speech recognition by adding one or more small auxiliary output layers modeling broad acoustic units, such as mono-phones or tied-state (often called senone) clusters. In scenarios with a limited amount of available adaptation data, most senones are usually rarely seen or not observed, and consequently the ability to model them in a new condition is often not fully exploited. With the original senone classification task as the primary task, and adding auxiliary mono-phone/senone-cluster classification as the secondary tasks, multi-task learning (MTL) is employed to adapt the DNN parameters. With the proposed MTL adaptation framework, we improve the learning ability of the original DNN structure, then enlarge the coverage of the acoustic space to deal with the unseen senone problem, and thus enhance the discrimination power of the adapted DNN models. Experimental results on the 20,000-word open vocabulary WSJ task demonstrate that the proposed framework consistently outperforms the conventional linear hidden layer adaptation schemes without MTL by providing 5.4% relative reduction in word error rate (WERR) with only 1 single adaptation utterance, and 10.7% WERR with 40 adaptation utterances against the un-adapted DNN models.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/huang15h_interspeech.pdf',6),(51,'Robust pitch estimation in noisy speech using ZTW and group delay function','Identification of pitch for speech signals recorded in noisy environments is a fundamental and long persistent problem in speech research. Several time domain based techniques attempt to exploit the periodic nature of the waveform using autocorrelation function and its variants. Other set of techniques utilize the harmonic structure in the spectral domain to identify pitch values. Either of these techniques suffer significant degradation in their performance in cases of noisy speech signals with low SNRs. The paper presents a robust technique to identify pitch values for speech signals. The proposed algorithm utilizes a speech analysis method called zero-time windowing (ZTW) where the signal is processed using a heavily decaying window, and the spectral characteristics are highlighted using the numerator of the group delay function. The amplitude contour of dominant resonances in the spectra are extracted, and processed further using a Gaussian window. The resulting contour reflects the energy profile of the signal which is utilized for estimation of the pitch values. The proposed algorithm is robust to degradations, and has been tested on several utterances with added noises. The algorithm exhibits significant increment in performance when compared to existing techniques.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/prasad15b_interspeech.pdf',6),(52,'Prosodic phrasing unique to the acquisition of L2 intonation — an analysis of L2 Japanese intonation by L1 Swedish learners','This paper examines the prosodic organization of L2 Japanese produced by L1 Swedish at the beginner level. Japanese and Swedish have been well studied for their prosodic structures and some well-defined prosodic phrases have been proposed. However, these existing prosodic phrases are found to be inadequate in analyzing L2 intonation seen as interlanguage. Instead, it consists of some unique phrasing showing the characteristics of interlanguage, i.e. a language that has its own system and it changes continuously during the acquisition process. Studies on interlanguage are mostly on grammar and not much is known about the acquisition of L2 intonation. The results reveal that the beginner level L2 intonation is characterized by many pauses that are inserted at every grammatical phrase boundary. Such a phasing is unique as interlanguage and presumably universal in the less fluent speech at the beginner level. While a typical prosodic phrasing in Japanese uses downstep to group APs to iPs, a typical phrasing in L2 Japanese produced by L1 Swedish uses upstep and some other patterns instead. They are considered to be L1 prosodic transfer.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/naganomadsen15_interspeech.pdf',6),(53,'A multi-region deep neural network model in speech recognition','This work proposes a new architecture for deep neural network training. Instead of having one cascade of fully connected hidden layers between the input features and the target output, the new architecture organizes hidden layers into several regions with each region having its own target. Regions communicate with each other during the training process by connections among intermediate hidden layers to share learned internal representations from their respective targets. They do not have to share the same input features. This paper presents the performance of acoustic models built using this architecture with speaker independent and dependent features. Experimental results are compared with not only the baseline DNN model, but also the ensemble DNN, unfolded RNN and stacked DNN. Experiments on the IARPA sponsored Babel tasks demonstrate improvements ranging from 0.8% to 2.7% absolute reduction in WER.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/cui15_interspeech.pdf',6),(54,'Tongue tracking in ultrasound images using eigentongue decomposition and artificial neural networks','This paper describes a machine learning approach for extracting automatically the tongue contour in ultrasound images. This method is developed in the context of visual articulatory biofeedback for speech therapy. The goal is to provide a speaker with an intuitive visualization of his/her tongue movement, in real-time, and with minimum human intervention. Contrary to most widely used techniques based on active contours, the proposed method aims at exploiting the information of all image pixels to infer the tongue contour. For that purpose, a compact representation of each image is extracted using a PCA-based decomposition technique (named EigenTongue). Artificial neural networks are then used to convert the extracted visual features into control parameters of a PCA-based tongue contour model. The proposed method is evaluated on 9 speakers, using data recorded with the ultrasound probe hold manually (as in the targeted application). Speaker-dependent experiments demonstrated the effectiveness of the proposed method (with an average error of ~1.3 mm when training from 80 manually annotated images), even when the tongue contour is poorly imaged. The performance was significantly lower in speaker-independent experiments ( i.e. when estimating contours on an unknown speaker), likely due to anatomical differences across speakers.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/fabre15_interspeech.pdf',6),(55,'Efficient use of DNN bottleneck features in generalized variable parameter HMMs for noise robust speech recognition','Recently a new approach to incorporate deep neural networks (DNN) bottleneck features into HMM based acoustic models using generalized variable parameter HMMs (GVP-HMMs) was proposed. As Gaussian component level polynomial interpolation is performed for each high dimensional DNN bottleneck feature vector at a frame level, conventional GVP-HMMs are computationally expensive to use in recognition time. To handle this problem, several approaches were exploited in this paper to efficiently use DNN bottleneck features in GVP-HMMs, including model selection techniques to optimally reduce the polynomial degrees; an efficient GMM based bottleneck feature clustering scheme; more compact GVP-HMM trajectory modelling for model space tied linear transformations. These improvements gave a total of 16 time speed up in decoding time over conventional GVP-HMMs using a uniformly assigned polynomial degree. Significant error rate reductions of 15.6% relative were obtained over the baseline tandem HMM system on the secondary microphone channel condition of Aurora 4 task. Consistent improvements were also obtained on other subsets.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/su15b_interspeech.pdf',6),(56,'The relationship between acoustic and perceived intraspeaker variability in voice quality','Little is known about intraspeaker changes in voice across changing speaking situations in everyday life. In this study, we examined acoustic variations between and within 5 talkers and their effect on the likelihood that voice samples would not be identified as coming from the same talker. Talkers were drawn from a large database recorded to capture everyday variations in vocal characteristics. Nine samples of /a/, recorded on three different days, were examined for each talker. Acoustic characteristics were estimated using VoiceSauce and analysis-by-synthesis, and listeners judged whether pairs of voices came from the same or two different talkers. Results indicate that interspeaker variability in voice quality exceeds intraspeaker variability, but differences are smaller than expected. As predicted by models that treat voice quality as an auditory pattern, the acoustic attributes associated with incorrect “different speaker” responses varied from talker to talker, depending on the particular characteristics of the voice in question.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/kreiman15_interspeech.pdf',6),(57,'Online Lombard adaptation in incremental speech synthesis','The `Lombard effect\' consists of various speech adaptation mechanisms human speakers use involuntarily to counter influences that a noisy environment has on their speech intelligibility. These adaptations are highly dependent on the characteristics of the noise and happen rapidly. Modelling the effect for the output side of speech interfaces is therefore difficult: the noise characteristics need to be evaluated continuously and speech synthesis adaptations need to take effect immediately. This paper describes and evaluates an online system consisting of a module that analyses the acoustic environment and a module that adapts the speech parameters of an incremental speech synthesis system in a timely manner. In an evaluation with human listeners the system had a similar effect on intelligibility as had human speakers in offline studies. Furthermore, during noise the Lombard-adapted speech was rated more natural than standard speech.','https://www.isca-speech.org/archive/pdfs/interspeech_2015/rottschafer15_interspeech.pdf',6),(58,'Improving Automatic Recognition of Aphasic Speech with AphasiaBank','Automatic recognition of aphasic speech is challenging due to various\nspeech-language impairments associated with aphasia as well as a scarcity\nof training data appropriate for this speaker population. AphasiaBank,\na shared database of multimedia interactions primarily used by clinicians\nto study aphasia, offers a promising source of data for Deep Neural\nNetwork acoustic modeling. In this paper, we establish the first large-vocabulary\ncontinuous speech recognition baseline on AphasiaBank and study recognition\naccuracy as a function of diagnoses. We investigate several out-of-domain\nadaptation methods and show that AphasiaBank data can be leveraged\nto significantly improve the recognition rate on a smaller aphasic\nspeech corpus. This work helps broaden the understanding of aphasic\nspeech recognition, demonstrates the potential of AphasiaBank, and\nguides researchers who wish to use this database for their own work.','https://www.isca-speech.org/archive/pdfs/interspeech_2016/le16b_interspeech.pdf',7),(59,'Improved Depiction of Tissue Boundaries in Vocal Tract Real-Time MRI Using Automatic Off-Resonance Correction','Real-time magnetic resonance imaging (RT-MRI) is a powerful tool to\nstudy the dynamics of vocal tract shaping during speech production.\nThe dynamic articulators of interest include the surfaces of the lips,\ntongue, hard palate, soft palate, and pharyngeal airway. All of these\nare located at air-tissue interfaces and are vulnerable to MRI off-resonance\neffect due to magnetic susceptibility. In RT-MRI using spiral or radial\nscanning, this appears as a signal loss or blurring in images and may\nimpair the analysis of dynamic speech data. We apply an automatic off-resonance\nartifact correction method to speech RT-MRI data in order to enhance\nthe sharpness of air-tissue boundaries. We demonstrate the improvement\nqualitatively and using an image sharpness metric offering an improved\ntool for speech science research.','https://www.isca-speech.org/archive/pdfs/interspeech_2016/lim16b_interspeech.pdf',7),(60,'Investigation of Speed-Accuracy Tradeoffs in Speech Production Using Real-Time Magnetic Resonance Imaging','Motor actions in speech production are both rapid and highly dexterous,\neven though speed and accuracy are often thought to conflict. Fitts’\nlaw has served as a rigorous formulation of the fundamental speed-accuracy\ntradeoff in other domains of human motor action, but has not been directly\nexamined with respect to speech production. This paper examines Fitts’\nlaw in speech articulation kinematics by analyzing USC-TIMIT, a large\ndatabase of real-time magnetic resonance imaging data of speech production.\nThis paper also addresses methodological challenges in applying Fitts-style\nanalysis, including the definition and operational measurement of key\nvariables in real-time MRI data. Results suggest high variability in\nthe task demands associated with targeted articulatory kinematics,\nas well as a clear tradeoff between speed and accuracy for certain\ntypes of speech production actions. Consonant targets, and particularly\nthose following vowels, show the strongest evidence of this tradeoff,\nwith correlations as high as 0.71 between movement time and difficulty.\nOther speech actions seem to challenge Fitts’ law. Results are\ndiscussed with respect to limitations of Fitts’ law in the context\nof speech production, as well as future improvements and applications.','https://www.isca-speech.org/archive/pdfs/interspeech_2016/lammert16_interspeech.pdf',7),(61,'Short Utterance Variance Modelling and Utterance Partitioning for PLDA Speaker Verification','This paper analyses the short utterance probabilistic linear discriminant\nanalysis (PLDA) speaker verification with utterance partitioning and\nshort utterance variance (SUV) modelling approaches. Experimental studies\nhave found that instead of using single long-utterance as enrolment\ndata, if long enrolled-utterance is partitioned into multiple short\nutterances and average of short utterance i-vectors is used as enrolled\ndata, that improves the Gaussian PLDA (GPLDA) speaker verification.\nThis is because short utterance i-vectors have speaker, session and\nutterance variations, and utterance-partitioning approach compensates\nthe utterance variation. Subsequently, SUV-PLDA is also studied with\nutterance partitioning approach, and utterance-partitioning-based SUV-GPLDA\nsystem shows relative improvement of 9% and 16% in EER for NIST 2008\nand NIST 2010 truncated 10sec-10sec evaluation condition as utterance-partitioning\napproach compensates the utterance variation and SUV modelling approach\ncompensates the mismatch between full-length development data and short-length\nevaluation data.','https://www.isca-speech.org/archive/pdfs/interspeech_2016/kanagasundaram16_interspeech.pdf',7),(62,'A Fast and Accurate Fundamental Frequency Estimator Using Recursive Moving Average Filters','We propose a fundamental frequency (F0) estimation method which is\nfast, accurate and suitable for real-time use. While the proposed method\nis based on the same framework as DIO [1, 2], it has two clear differences:\nit uses RMA (Recursive Moving Average) filters for attenuating high\norder harmonics, and the period detector is designed to work well even\nfor signals which contain some higher harmonics. Effect of trace-back\nduration of post-processing was also examined. Evaluation experiments\nusing natural speech databases showed that the accuracy of the proposed\nmethod was better than DIO, SWIPE\' [3] and YIN [4] and computation\nspeed was the fastest compared to those existing methods.','https://www.isca-speech.org/archive/pdfs/interspeech_2016/daido16_interspeech.pdf',7),(63,'Test-Retest Repeatability of Articulatory Strategies Using Real-Time Magnetic Resonance Imaging','Real-time magnetic resonance imaging (rtMRI) provides information about\nthe dynamic shaping of the vocal tract during speech production. This\npaper introduces and evaluates a method for quantifying articulatory\nstrategies using rtMRI. The method decomposes the formation and release\nof a constriction in the vocal tract into the contributions of individual\narticulators such as the jaw, tongue, lips, and velum. The method uses\nan anatomically guided factor analysis and dynamical principles from\nthe framework of Task Dynamics. We evaluated the method within a test-retest\nrepeatability framework. We imaged healthy volunteers (n = 8, 4 females,\n4 males) in two scans on the same day and quantified inter-study agreement\nwith the intraclass correlation coefficient and mean within-subject\nstandard deviation. The evaluation established a limit on effect size\nand intra-group differences in articulatory strategy which can be studied\nusing the method.','https://www.isca-speech.org/archive/pdfs/interspeech_2017/sorensen17b_interspeech.pdf',8),(64,'Google’s Next-Generation Real-Time Unit-Selection Synthesizer Using Sequence-to-Sequence LSTM-Based Autoencoders','A neural network model that significant improves unit-selection-based\nText-To-Speech synthesis is presented. The model employs a sequence-to-sequence\nLSTM-based autoencoder that compresses the acoustic and linguistic\nfeatures of each unit to a fixed-size vector referred to as an  embedding.\nUnit-selection is facilitated by formulating the target cost as an\nL','https://www.isca-speech.org/archive/pdfs/interspeech_2017/wan17_interspeech.pdf',8),(65,'Exploring the Use of Significant Words Language Modeling for Spoken Document Retrieval','Owing to the rapid global access to tremendous amounts of multimedia\nassociated with speech information on the Internet, spoken document\nretrieval (SDR) has become an emerging application recently. Apart\nfrom much effort devoted to developing robust indexing and modeling\ntechniques for spoken documents, a recent line of research targets\nat enriching and reformulating query representations in an attempt\nto enhance retrieval effectiveness. In practice, pseudo-relevance feedback\nis by far the most prevalent paradigm for query reformulation, which\nassumes that top-ranked feedback documents obtained from the initial\nround of retrieval are potentially relevant and can be exploited to\nreformulate the original query. Continuing this line of research, the\npaper presents a novel modeling framework, which aims at discovering\nsignificant words occurring in the feedback documents, to infer an\nenhanced query language model for SDR. Formally, the proposed framework\ntargets at extracting the essential words representing a common notion\nof relevance (i.e., the significant words which occur in almost all\nof the feedback documents), so as to deduce a new query language model\nthat captures these significant words and meanwhile modulates the influence\nof both highly frequent words and too specific words. Experiments conducted\non a benchmark SDR task demonstrate the performance merits of our proposed\nframework.','https://www.isca-speech.org/archive/pdfs/interspeech_2017/chen17l_interspeech.pdf',8),(66,'Cross-Modal Analysis Between Phonation Differences and Texture Images Based on Sentiment Correlations','Motivated by the success of speech characteristics representation by\ncolor attributes, we analyzed the cross-modal sentiment correlations\nbetween voice source characteristics and textural image characteristics.\nFor the analysis, we employed vowel sounds with representative three\nphonation differences (modal, creaky and breathy) and 36 texture images\nwith 36 semantic attributes (e.g., banded, cracked and scaly) annotated\none semantic attribute for each texture. By asking 40 subjects to select\nthe most fitted textures from 36 figures with different textures after\nlistening 30 speech samples with different phonations, we measured\nthe correlations between acoustic parameters showing voice source variations\nand the parameters of selected textural image differences showing coarseness,\ncontrast, directionality, busyness, complexity and strength. From the\ntexture classifications, voice characteristics can be roughly characterized\nby textural differences: modal — gauzy, banded and smeared, creaky\n— porous, crystalline, cracked and scaly, breathy — smeared,\nfreckled and stained. We have also found significant correlations between\nvoice source acoustic parameters and textural parameters. These correlations\nsuggest the possibility of cross-modal mapping between voice source\ncharacteristics and textural parameters, which enables visualization\nof speech information with source variations reflecting human sentiment\nperception.','https://www.isca-speech.org/archive/pdfs/interspeech_2017/kyaw17_interspeech.pdf',8),(67,'Creating a Voice for  MiRo, the World’s First Commercial Biomimetic Robot','This paper introduces  MiRo — the world’s first commercial\nbiomimetic robot — and describes how its vocal system was designed\nusing a real-time parametric general-purpose mammalian vocal synthesiser\ntailored to the specific physical characteristics of the robot.  MiRo’s\ncapabilities will be demonstrated live during the hands-on interactive\n‘Show & Tell’ session at INTERSPEECH-2017.','https://www.isca-speech.org/archive/pdfs/interspeech_2017/moore17_interspeech.pdf',8),(68,'Speech Recognition and Understanding on Hardware-Accelerated DSP','A smart home controller that responds to natural language input is\ndemonstrated on an Intel embedded processor. This device contains two\nDSP cores and a neural network co-processor which share 4MB SRAM. An\nembedded configuration of the Intel RealSpeech speech recognizer and\nintent extraction engine runs on the DSP cores with neural network\noperations offloaded to the co-processor. The prototype demonstrates\nthat continuous speech recognition and understanding is possible on\nhardware with very low power consumption. As an example application,\ncontrol of lights in a home via natural language is shown. An Intel\ndevelopment kit is demonstrated together with a set of tools. Conference\nattendees are encouraged to interact with the demo and development\nsystem.','https://www.isca-speech.org/archive/pdfs/interspeech_2017/stemmer17_interspeech.pdf',8),(69,'The Kaldi OpenKWS System: Improving Low Resource Keyword Search','The IARPA BABEL program has stimulated worldwide research in keyword\nsearch technology for low resource languages, and the NIST OpenKWS\nevaluations are the de facto benchmark test for such capabilities.\nThe 2016 OpenKWS evaluation featured Georgian speech, and had 10 participants\nfrom across the world. This paper describes the Kaldi system developed\nto assist IARPA in creating a competitive baseline against which participants\nwere evaluated, and to provide a truly open source system to all participants\nto support their research. This system handily met the BABEL program\ngoals of 0.60 ATWV and 50% WER, achieving 0.70 ATWV and 38% WER with\na single ASR system, i.e.  without ASR system combination. All except\none OpenKWS participant used Kaldi components in their submissions,\ntypically in conjunction with system combination. This paper therefore\ncomplements all other OpenKWS-based papers.','https://www.isca-speech.org/archive/pdfs/interspeech_2017/trmal17_interspeech.pdf',8),(70,'Autoencoder Based Domain Adaptation for Speaker Recognition Under Insufficient Channel Information','In real-life conditions, mismatch between development and test domain\ndegrades speaker recognition performance. To solve the issue, many\nresearchers explored domain adaptation approaches using matched in-domain\ndataset. However, adaptation would be not effective if the dataset\nis insufficient to estimate channel variability of the domain. In this\npaper, we explore the problem of performance degradation under such\na situation of insufficient channel information. In order to exploit\nlimited in-domain dataset effectively, we propose an unsupervised domain\nadaptation approach using Autoencoder based Domain Adaptation (AEDA).\nThe proposed approach combines an autoencoder with a denoising autoencoder\nto adapt resource-rich development dataset to test domain. The proposed\ntechnique is evaluated on the Domain Adaptation Challenge 13 experimental\nprotocols that is widely used in speaker recognition for domain mismatched\ncondition. The results show significant improvements over baselines\nand results from other prior studies.','https://www.isca-speech.org/archive/pdfs/interspeech_2017/shon17_interspeech.pdf',8),(71,'Automatic Explanation Spot Estimation Method Targeted at Text and Figures in Lecture Slides','Because of the spread of the Internet in recent years, e-learning,\nwhich is a form of learning through the Internet, has been used in\nschool education. Many lecture videos delivered at The Open University\nof Japan show lecturers and lecture slides alternately. In such video\nstyle, it is hard to understand where on the slide the lecturer is\nexplaining. In this paper, we examined methods to automatically estimate\nspots where the lecturer explains on the slide using lecture speech\nand slide data. This technology is expected to help learners to study\nthe lectures. For itemized text slides, using DTW with word embedding\nbased distance, we obtained higher estimation accuracy than a previous\nwork. For slides containing figures, we estimated explanation spots\nusing image classification results and text in the charts. In addition,\nwe modified the lecture browsing system to indicate estimation results\non slides, and investigated the usefulness of indicating explanation\nspots by subjective evaluation with the system.','https://www.isca-speech.org/archive/pdfs/interspeech_2017/tsujimura17_interspeech.pdf',8),(72,'Prosody Control of Utterance Sequence for Information Delivering','We propose a conversational speech synthesis system in which the prosodic\nfeatures of each utterance are controlled throughout the entire input\ntext. We have developed a “news-telling system,” which\ndelivered news articles through spoken language. The speech synthesis\nsystem for the news-telling should be able to highlight utterances\ncontaining noteworthy information in the article with a particular\nway of speaking so as to impress them on the users. To achieve this,\nwe introduced role and position features of the individual utterances\nin the article into the control parameters for prosody generation throughout\nthe text. We defined three categories for the role feature: a nucleus\n(which is assigned to the utterance including the noteworthy information),\na front satellite (which precedes the nucleus) and a rear satellite\n(which follows the nucleus). We investigated how the prosodic features\ndiffered depending on the role and position features through an analysis\nof news-telling speech data uttered by a voice actress. We designed\nthe speech synthesis system on the basis of a deep neural network having\nthe role and position features added to its input layer. Objective\nand subjective evaluation results showed that introducing those features\nwas effective in the speech synthesis for the information delivering.','https://www.isca-speech.org/archive/pdfs/interspeech_2017/fukuoka17_interspeech.pdf',8),(73,'Efficient Language Model Adaptation with Noise Contrastive Estimation and Kullback-Leibler Regularization','Many language modeling (LM) tasks have limited in-domain data for training. Exploiting out-of-domain data while retaining the relevant in-domain statistics is a desired property in these scenarios. Kullback-Leibler Divergence (KLD) regularization is a popular method for acoustic model (AM) adaptation. KLD regularization assumes that the last layer is a softmax that fully activates the targets of both in-domain and out-of-domain models. Unfortunately, this softmax activation is computationally prohibitive for language modeling where the number of output classes is large, typically 50k to 100K, but may even exceed 800k in some cases. The computational bottleneck of the softmax during LM training can be reduced by an order of magnitude using techniques such as noise contrastive estimation (NCE), which replaces the cross-entropy loss function with a binary classification problem between the target output and random noise samples. In this work we combine NCE and KLD regularization and offer a fast domain adaptation method for LM training, while also retaining important attributes of the original NCE, such as self-normalization. We show on a medical domain-adaptation task that our method improves perplexity by 10.1% relative to a strong LSTM baseline.','https://www.isca-speech.org/archive/pdfs/interspeech_2018/andresferrer18_interspeech.pdf',9),(74,'Conditional End-to-End Audio Transforms','We present an end-to-end method for transforming audio from one style to another. For the case of speech, by conditioning on speaker identities, we can train a single model to transform words spoken by multiple people into multiple target voices. For the case of music, we can specify musical instruments and achieve the same result. Architecturally, our method is a fully-differentiable sequence-to-sequence model based on convolutional and hierarchical recurrent neural networks. It is designed to capture long-term acoustic dependencies, requires minimal post-processing and produces realistic audio transforms. Ablation studies confirm that our model can separate acoustic properties from musical and language content at different receptive fields. Empirically, our method achieves competitive performance on community-standard datasets.','https://www.isca-speech.org/archive/pdfs/interspeech_2018/haque18_interspeech.pdf',9),(75,'Paired Phone-Posteriors Approach to ESL Pronunciation Quality Assessment','This work proposes to incorporate paired phone-posteriors as input features into a neural net (NN) model for assessing ESL learner’s pronunciation quality. In this work, posteriors of forty phones, instead of several thousand sub-phonemic senones, are used to circumvent the sparsity issues in NN training. Phone posteriors are assembled with their corresponding senone posteriors estimated via a speaker-independent, DNN-based acoustic model, trained with standard American English speech data (i.e., Wall Street Journal database). Phone posteriors of both reference(standard American English speaker) and test speaker are paired together as augmented input feature vectors to train an NN based, 2-class, i.e., native vs nonnative speaker, classiﬁer. The Goodness of Pronunciation (GOP), a proven effective measure, is used as the baseline for comparison. The binary NN classiﬁer trained with such features achieves a high classification accuracy of 89.6% on native and non-native speakers’ data. The classiﬁer also shows a better equal error rate (EER) than the GOP-based baseline classiﬁer in either phone or word level pronunciation, i.e., at phone level from 18.3% to 6.2% and at word level from 12.98% to 2.54%.','https://www.isca-speech.org/archive/pdfs/interspeech_2018/xiao18b_interspeech.pdf',9),(76,'Deep Extractor Network for Target Speaker Recovery from Single Channel Speech Mixtures','Speaker-aware source separation methods are promising workarounds for major difficulties such as arbitrary source permutation and unknown number of sources. However, it remains challenging to achieve satisfying performance provided a very short available target speaker utterance (anchor). Here we present a novel \"deep extractor network\" which creates an extractor point for the target speaker in a canonical high dimensional embedding space and pulls together the time-frequency bins corresponding to the target speaker. The proposed model is different from prior works that the carnonical embedding space encodes knowledges of both the anchor and the mixture during training phase: first, embeddings for the anchor and mixture speech are separately constructed in a primary embedding space and then combined as an input to feed-forward layers to transform to a carnonical embedding space which we discover more stable than the primary one. Experimental results show that given a very short utterance, the proposed model can efficiently recover high quality target speech from a mixture, which outperforms various baseline models, with 5.2% and 6.6% relative improvements in SDR and PESQ respectively compared with a baseline oracle deep attracor model. Meanwhile, we show it can be generalized well to more than one interfering speaker.','https://www.isca-speech.org/archive/pdfs/interspeech_2018/wang18d_interspeech.pdf',9),(77,'End-to-End Multi-Speaker Speech Recognition Using Speaker Embeddings and Transfer Learning','This paper presents our latest investigation on end-to-end automatic\nspeech recognition (ASR) for overlapped speech. We propose to train\nan end-to-end system conditioned on speaker embeddings and further\nimproved by transfer learning from clean speech. This proposed framework\ndoes not require any parallel non-overlapped speech materials and is\nindependent of the number of speakers. Our experimental results on\noverlapped speech datasets show that joint conditioning on speaker\nembeddings and transfer learning significantly improves the ASR performance.','https://www.isca-speech.org/archive/pdfs/interspeech_2019/denisov19_interspeech.pdf',10),(78,'A Preliminary Study of Charismatic Speech on YouTube: Correlating Prosodic Variation with Counts of Subscribers, Views and Likes','This paper is a first investigation into the influence of the pitch\nrange and the intensity variation on the number of subscribers, views\nand likes of YouTube Creators. A total of ten minutes of speech material\nfrom five English and five North-American YouTubers was analyzed. The\nresults for pitch range and intensity variation suggest that an increase\nin both parameters results in higher subscriber counts. For views,\nthere was no influence of pitch range, but an increase in intensity\nvariation results in a lower number of views. Pitch range and intensity\nvariation had no influence on the like count. Furthermore, both origin\nand gender had an influence on the results. Ultimately, this study\nwill provide further information for the phonetic research of charisma\n(i.e., the perceived charm, competence, power, and persuasiveness of\na speaker), as it is suspected that the acoustic features that have\nso far been connected to charisma also play an important role in the\nsuccess of a YouTuber and their channel.','https://www.isca-speech.org/archive/pdfs/interspeech_2019/berger19_interspeech.pdf',10),(79,'Articulatory Characteristics of Secondary Palatalization in Romanian Fricatives','This study explores the articulatory characteristics of plain and palatalized\nfricatives in Romanian. Based on earlier acoustic findings, we hypothesize\nthat there are differences in tongue raising and fronting depending\non the primary place of articulation, with more subtle gestures produced\nin the vicinity of the palatal area. We also predict more individual\nvariation in the realization of secondary palatalization in postalveolars,\nbased on general cross-linguistic patterns.','https://www.isca-speech.org/archive/pdfs/interspeech_2019/spinu19_interspeech.pdf',10),(80,'Using Speech Production Knowledge for Raw Waveform Modelling Based Styrian Dialect Identification','This paper addresses the Styrian Dialect sub-challenge of the INTERSPEECH\n2019 Computational Paralinguistics Challenge. We treat this challenge\nas dialect identification with no linguistic resources/knowledge and\nwith limited acoustic resources, and develop end-to-end raw waveform\nmodelling based methods that incorporate knowledge related to speech\nproduction. In this direction, we investigate two methods: (a) modelling\nthe signals after source system decomposition and (b) transferring\nknowledge from articulatory feature models trained on English language.\nOur investigations show that the proposed approaches on the ComParE\n2019 Styrian dialect data yield systems that perform better than low\nlevel descriptor-based and bag-of-audio-word representation based approaches\nand comparable to sequence-to-sequence auto-encoder based approach.','https://www.isca-speech.org/archive/pdfs/interspeech_2019/dubagunta19_interspeech.pdf',10),(81,'Optimizing Speech-Input Length for Speaker-Independent Depression Classification','Machine learning models for speech-based depression classification\noffer promise for health care applications. Despite growing work on\ndepression classification, little is understood about how the length\nof speech-input impacts model performance. We analyze results for speaker-independent\ndepression classification using a corpus of over 1400 hours of speech\nfrom a human-machine health screening application. We examine performance\nas a function of response input length for two NLP systems that differ\nin overall performance.','https://www.isca-speech.org/archive/pdfs/interspeech_2019/rutowski19_interspeech.pdf',10),(82,'Neural Network-Based Modeling of Phonetic Durations','A deep neural network (DNN)-based model has been developed to predict\nnon-parametric distributions of durations of phonemes in specified\nphonetic contexts and used to explore which factors influence durations\nmost. Major factors in US English are pre-pausal lengthening, lexical\nstress, and speaking rate. The model can be used to check that text-to-speech\n(TTS) training speech follows the script and words are pronounced as\nexpected. Duration prediction is poorer with training speech for automatic\nspeech recognition (ASR) because the training corpus typically consists\nof single utterances from many speakers and is often noisy or casually\nspoken. Low probability durations in ASR training material nevertheless\nmostly correspond to non-standard speech, with some having disfluencies.\nChildren’s speech is disproportionately present in these utterances,\nsince children show much more variation in timing.','https://www.isca-speech.org/archive/pdfs/interspeech_2019/wei19_interspeech.pdf',10),(83,'Study of the Performance of Automatic Speech Recognition Systems in Speakers with Parkinson’s Disease','Parkinson’s Disease (PD) affects motor capabilities of patients,\nwho in some cases need to use human-computer assistive technologies\nto regain independence. The objective of this work is to study in detail\nthe differences in error patterns from state-of-the-art Automatic Speech\nRecognition (ASR) systems on speech from people with and without PD.\nTwo different speech recognizers (attention-based end-to-end and Deep\nNeural Network - Hidden Markov Models hybrid systems) were trained\non a Spanish language corpus and subsequently tested on speech from\n43 speakers with PD and 46 without PD. The differences related to error\nrates, substitutions, insertions and deletions of characters and phonetic\nunits between the two groups were analyzed, showing that the word error\nrate is 27% higher in speakers with PD than in control speakers, with\na moderated correlation between that rate and the developmental stage\nof the disease. The errors were related to all manner classes, and\nwere more pronounced in the vowel /u/. This study is the first to evaluate\nASR systems’ responses to speech from patients at different stages\nof PD in Spanish. The analyses showed general trends but individual\nspeech deficits must be studied in the future when designing new ASR\nsystems for this population.','https://www.isca-speech.org/archive/pdfs/interspeech_2019/morovelazquez19_interspeech.pdf',10),(84,'Variational Domain Adversarial Learning for Speaker Verification','Domain mismatch refers to the problem in which the distribution of\ntraining data differs from that of the test data. This paper proposes\na variational domain adversarial neural network (VDANN), which consists\nof a variational autoencoder (VAE) and a domain adversarial neural\nnetwork (DANN), to reduce domain mismatch. The DANN part aims to retain\nspeaker identity information and learn a feature space that is robust\nagainst domain mismatch, while the VAE part is to impose variational\nregularization on the learned features so that they follow a Gaussian\ndistribution. Thus, the representation produced by VDANN is not only\nspeaker discriminative and domain-invariant but also Gaussian distributed,\nwhich is essential for the standard PLDA backend. Experiments on both\nSRE16 and SRE18-CMN2 show that VDANN outperforms the Kaldi baseline\nand the standard DANN. The results also suggest that VAE regularization\nis effective for domain adaptation.','https://www.isca-speech.org/archive/pdfs/interspeech_2019/tu19_interspeech.pdf',10),(85,'An Investigation of the Target Approximation Model for Tone Modeling and Recognition in Continuous Mandarin Speech','The complex f','https://www.isca-speech.org/archive/pdfs/interspeech_2020/gao20c_interspeech.pdf',11),(86,'What the Future Brings: Investigating the Impact of Lookahead for Incremental Neural TTS','In incremental text to speech synthesis (iTTS), the synthesizer produces\nan audio output before it has access to the entire input sentence.\nIn this paper, we study the behavior of a neural sequence-to-sequence\nTTS system when used in an incremental mode, i.e. when generating speech\noutput for token n, the system has access to  n+k tokens from the text\nsequence. We first analyze the impact of this incremental policy on\nthe evolution of the encoder representations of token n for different\nvalues of k (the lookahead parameter). The results show that, on average,\ntokens travel 88% of the way to their full context representation with\na one-word lookahead and 94% after 2 words. We then investigate which\ntext features are the most influential on the evolution towards the\nfinal representation using a random forest analysis. The results show\nthat the most salient factors are related to token length. We finally\nevaluate the effects of lookahead k at the decoder level, using a MUSHRA\nlistening test. This test shows results that contrast with the above\nhigh figures: speech synthesis quality obtained with 2 word-lookahead\nis significantly lower than the one obtained with the full sentence.','https://www.isca-speech.org/archive/pdfs/interspeech_2020/stephenson20_interspeech.pdf',11),(87,'On the Robustness and Training Dynamics of Raw Waveform Models','We investigate the robustness and training dynamics of raw waveform\nacoustic models for automatic speech recognition (ASR). It is known\nthat the first layer of such models learn a set of filters, performing\na form of time-frequency analysis. This layer is liable to be under-trained\nowing to gradient vanishing, which can negatively affect the network\nperformance. Through a set of experiments on TIMIT, Aurora-4 and WSJ\ndatasets, we investigate the training dynamics of the first layer by\nmeasuring the evolution of its average frequency response over different\nepochs. We demonstrate that the network efficiently learns an optimal\nset of filters with a high spectral resolution and the dynamics of\nthe first layer highly correlates with the dynamics of the cross entropy\n(CE) loss and word error rate (WER). In addition, we study the robustness\nof raw waveform models in both matched and mismatched conditions. The\naccuracy of these models is found to be comparable to, or better than,\ntheir MFCC-based counterparts in matched conditions and notably improved\nby using a better alignment. The role of raw waveform normalisation\nwas also examined and up to 4.3% absolute WER reduction in mismatched\nconditions was achieved.','https://www.isca-speech.org/archive/pdfs/interspeech_2020/loweimi20_interspeech.pdf',11),(88,'Spoofing Attack Detection Using the Non-Linear Fusion of Sub-Band Classifiers','The threat of spoofing can pose a risk to the reliability of automatic\nspeaker verification. Results from the biannual ASVspoof evaluations\nshow that effective countermeasures demand front-ends designed specifically\nfor the detection of spoofing artefacts. Given the diversity in spoofing\nattacks, ensemble methods are particularly effective. The work in this\npaper shows that a bank of very simple classifiers, each with a front-end\ntuned to the detection of different spoofing attacks and combined at\nthe score level through non-linear fusion, can deliver superior performance\nthan more sophisticated ensemble solutions that rely upon complex neural\nnetwork architectures. Our comparatively simple approach outperforms\nall but 2 of the 48 systems submitted to the logical access condition\nof the most recent ASVspoof 2019 challenge.','https://www.isca-speech.org/archive/pdfs/interspeech_2020/tak20_interspeech.pdf',11),(89,'Classification of Manifest Huntington Disease Using Vowel Distortion Measures','Huntington disease (HD) is a fatal autosomal dominant neurocognitive\ndisorder that causes cognitive disturbances, neuropsychiatric symptoms,\nand impaired motor abilities (e.g., gait, speech, voice). Due to its\nprogressive nature, HD treatment requires ongoing clinical monitoring\nof symptoms. Individuals with the Huntington gene mutation, which causes\nHD, may exhibit a range of speech symptoms as they progress from premanifest\nto manifest HD. Speech-based passive monitoring has the potential to\naugment clinical information by more continuously tracking manifestation\nsymptoms. Differentiating between premanifest and manifest HD is an\nimportant yet understudied problem, as this distinction marks the need\nfor increased treatment. In this work we present the first demonstration\nof how changes in speech can be measured to differentiate between premanifest\nand manifest HD. To do so, we focus on one speech symptom of HD: distorted\nvowels. We introduce a set of Filtered Vowel Distortion Measures (FVDM)\nwhich we extract from read speech. We show that FVDM, coupled with\nfeatures from existing literature, can differentiate between premanifest\nand manifest HD with 80% accuracy.','https://www.isca-speech.org/archive/pdfs/interspeech_2020/romana20_interspeech.pdf',11),(90,'TMT: A Transformer-Based Modal Translator for Improving Multimodal Sequence Representations in Audio Visual Scene-Aware Dialog','Audio Visual Scene-aware Dialog (AVSD) is a task to generate responses\nwhen discussing about a given video. The previous state-of-the-art\nmodel shows superior performance for this task using Transformer-based\narchitecture. However, there remain some limitations in learning better\nrepresentation of modalities. Inspired by Neural Machine Translation\n(NMT), we propose the Transformer-based Modal Translator (TMT) to learn\nthe representations of the source modal sequence by translating the\nsource modal sequence to the related target modal sequence in a supervised\nmanner. Based on Multimodal Transformer Networks (MTN), we apply TMT\nto video and dialog, proposing MTN-TMT for the video-grounded dialog\nsystem. On the AVSD track of the Dialog System Technology Challenge\n7, MTN-TMT outperforms the MTN and other submission models in both\nVideo and Text task and Text Only task. Compared with MTN, MTN-TMT\nimproves all metrics, especially, achieving relative improvement up\nto 14.1% on CIDEr.','https://www.isca-speech.org/archive/pdfs/interspeech_2020/li20ea_interspeech.pdf',11),(91,'Towards One Model to Rule All: Multilingual Strategy for Dialectal Code-Switching Arabic ASR','With the advent of globalization, there is an increasing demand for\nmultilingual automatic speech recognition (ASR), handling language\nand dialectal variation of spoken content. Recent studies show its\nefficacy over monolingual systems. In this study, we design a large\nmultilingual end-to-end ASR using self-attention based conformer architecture.\nWe trained the system using Arabic (Ar), English (En) and French (Fr)\nlanguages. We evaluate the system performance handling: (i) monolingual\n(Ar, En and Fr); (ii) multi-dialectal (Modern Standard Arabic, along\nwith dialectal variation such as Egyptian and Moroccan); (iii) code-switching\n— cross-lingual (Ar-En/Fr) and dialectal (MSA-Egyptian dialect)\ntest cases, and compare with current state-of-the-art systems. Furthermore,\nwe investigate the influence of different embedding/character representations\nincluding character vs word-piece; shared vs distinct input symbol\nper language. Our findings demonstrate the strength of such a model\nby outperforming state-of-the-art monolingual dialectal Arabic and\ncode-switching Arabic ASR.','https://www.isca-speech.org/archive/pdfs/interspeech_2021/chowdhury21_interspeech.pdf',12),(92,'Tied & Reduced RNN-T Decoder','Previous works on the Recurrent Neural Network-Transducer (RNN-T) models\nhave shown that, under some conditions, it is possible to simplify\nits prediction network with little or no loss in recognition accuracy\n[1, 2, 3]. This is done by limiting the context size of previous labels\nand/or using a simpler architecture for its layers instead of LSTMs.\nThe benefits of such changes include reduction in model size, faster\ninference and power savings, which are all useful for on-device applications.','https://www.isca-speech.org/archive/pdfs/interspeech_2021/botros21_interspeech.pdf',12),(93,'Image-Based Assessment of Jaw Parameters and Jaw Kinematics for Articulatory Simulation: Preliminary Results','Correcting the deficits in jaw movements have often been ignored in\nassessment and treatment of speech disorders. A robotic simulation\nis being developed to facilitate Speech Language Pathologists to demonstrate\nthe movement of jaw, tongue and teeth during production of speech sounds,\nas a part of a larger study. Profiling of jaw movement is an important\naspect of articulatory simulation. The present study attempts to develop\na simple and efficient technique for deriving the jaw parameters and\nusing them to simulate jaw movements through inverse kinematics.','https://www.isca-speech.org/archive/pdfs/interspeech_2021/abraham21_interspeech.pdf',12),(94,'Leveraging Real-Time MRI for Illuminating Linguistic Velum Action','Velum actions are critical to differentiating oral and nasal sounds\nin spoken language; specifically in the latter, the velum is lowered\nto open the nasal port and allow nasal airflow. However, details on\nhow the velum is lowered for nasal production in speech are scarce.\nState-of-the-art real-time Magnetic Resonance Imaging (rtMRI) can directly\nimage the entirety of the moving vocal tract, providing spatiotemporal\nkinematic data of articulatory actions. Most instrumental studies of\nspeech production explore oral constriction actions such as lip or\ntongue movements. RtMRI makes possible a quantitative assessment of\nnon-oral and non-constriction actions, such as velum (and larynx) dynamics.\nThis paper illustrates articulatory aspects of consonant nasality,\nwhich have previously been inferred from acoustic or aerodynamic data.\nVelum actions are quantified in spatial and temporal domains: i) vertical\nand horizontal velum positions during nasal consonant production are\nquantified to measure, respectively, the degree of velum lowering and\nvelic opening, and ii) duration intervals for velum lowering, plateau,\nand raising are obtained to understand which portion of the velum action\nis lengthened to generate phonologically long nasality. Findings demonstrate\nthat velum action tracking using rtMRI can illuminate linguistic modulations\nof nasality strength and length.','https://www.isca-speech.org/archive/pdfs/interspeech_2021/oh21b_interspeech.pdf',12),(95,'Ethical and Technological Challenges of Conversational AI','Conversational AI (ConvAI) systems have applications ranging from personal\nassistance, health assistance to customer services. They have been\nin place since the first call centre agent went live in the late 1990s.\nMore recently, smart speakers and smartphones are powered with conversational\nAI with similar architecture as those from the 90s. On the other hand,\nresearch on ConvAI systems has made leaps and bounds in recent years\nwith sequence-to-sequence, generation-based models. Thanks to the advent\nof large scale pre-trained language models, state-of-the-art ConvAI\nsystems can generate surprisingly human-like responses to user queries\nin open domain conversations, known as chit-chat. However, these generation\nbased ConvAI systems are difficult to control and can lead to inappropriate,\nbiased and sometimes even toxic responses. In addition, unlike previous\nmodular conversational AI systems, it is also challenging to incorporate\nexternal knowledge into these models for task-oriented dialog scenarios\nsuch as personal assistance and customer services, and to maintain\nconsistency.','https://www.isca-speech.org/archive/interspeech_2021/fung21_interspeech.html',12),(96,'Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability','Emotional text-to-speech synthesis (ETTS) has seen much progress in\nrecent years. However, the generated voice is often not perceptually\nidentifiable by its intended emotion category. To address this problem,\nwe propose a new interactive training paradigm for ETTS, denoted as','https://www.isca-speech.org/archive/pdfs/interspeech_2021/liu21p_interspeech.pdf',12),(97,'Non-Intrusive Speech Quality Assessment with Transfer Learning and Subject-Specific Scaling','In communication systems, it is crucial to estimate the perceived quality\nof audio and speech. The industrial standards for many years have been\nPESQ, 3QUEST, and POLQA, which are intrusive methods. This restricts\nthe possibilities of using these metrics in real-world conditions,\nwhere we might not have access to the clean reference signal. In this\nwork, we develop a new non-intrusive metric based on crowd-sourced\ndata. We build a new speech dataset by combining publicly available\nspeech, noises, and reverberations. Then we follow the ITU P.808 recommendation\nto label the dataset with mean opinion scores (MOS). Finally, we train\na deep neural network to estimate the MOS from the speech data in a\nnon-intrusive way. We propose two novelties in our work. First, we\nexplore transfer learning by pre-training a model using a larger set\nof POLQA scores and finetuning with the smaller (and thus cheaper)\nhuman-labeled set. Secondly, we perform a subject-specific scaling\nin the MOS scores to adjust for their different subjective scales.\nOur model yields better accuracy than PESQ, POLQA, and other non-intrusive\nmethods when evaluated on the independent VCTK test set. We also report\nmisleading POLQA scores for reverberant speech.','https://www.isca-speech.org/archive/pdfs/interspeech_2021/nessler21_interspeech.pdf',12),(98,'Voicing Assimilations by French Speakers of German in Stop-Fricative Sequences','Voicing assimilations inside groups of obstruents occur in opposite\ndirections in French and German, where they are respectively regressive\nand progressive. The aim of the study is to investigate (1) whether\nnon native speakers (here French learners of German) are apt to acquire\nsubtle L2 specificities like assimilation direction, although they\nare not aware of their very existence, or (2) whether their productions\ndepend essentially upon other factors, in particular consonant place\nof articulation. To that purpose, a corpus made up of groups of obstruents\n(/t/ followed by /z/, /v/ or /f/) embedded into sentences has been\nrecorded by 16 French learners of German (beginners and advanced speakers).\nThe consonants are separated by a word or a syllable boundary. Results,\nderived from the analysis of consonant periodicity and duration, do\nnot stand for an acquisition of progressive assimilation, even by advanced\nspeakers, and do not show differences between the productions of advanced\nspeakers and beginners. On the contrary the boundary type and the consonant\nplace of articulation play an important role in the presence or absence\nof voicing inside obstruent groups. The role of phonetic, universal\nmechanisms against linguistic specific rules is discussed to interpret\nthe data.','https://www.isca-speech.org/archive/pdfs/interspeech_2021/bonneau21_interspeech.pdf',12),(99,'End-to-End Language Diarization for Bilingual Code-Switching Speech','We propose two end-to-end neural configurations for language diarization\non bilingual code-switching speech. The first, a BLSTM-E2E architecture,\nincludes a set of stacked bidirectional LSTMs to compute embeddings\nand incorporates the deep clustering loss to enforce grouping of languages\nbelonging to the same class. The second, an XSA-E2E architecture, is\nbased on an x-vector model followed by a self-attention encoder. The\nformer encodes frame-level features into segment-level embeddings while\nthe latter considers all those embeddings to generate a sequence of\nsegment-level language labels. We evaluated the proposed methods on\nthe dataset obtained from the shared task B in WSTCSMC 2020 and our\nhandcrafted simulated data from the SEAME dataset. Experimental results\nshow that our proposed XSA-E2E architecture achieved a relative improvement\nof 12.1% in equal error rate and a 7.4% relative improvement on accuracy\ncompared with the baseline algorithm in the WSTCSMC 2020 dataset. Our\nproposed XSA-E2E architecture achieved an accuracy of 89.84% with a\nbaseline of 85.60% on the simulated data derived from the SEAME dataset.','https://www.isca-speech.org/archive/pdfs/interspeech_2021/liu21d_interspeech.pdf',12),(100,'The LF Model in the Frequency Domain for Glottal Airflow Modelling Without Aliasing Distortion','Many of the commonly used voice source models are based on piecewise\nelementary functions defined in the time domain. The discrete-time\nimplementation of such models generally causes aliasing distortion,\nwhich make them less useful for certain applications. This paper presents\na method which eliminates this distortion. The key component of the\nproposed method is the frequency domain description of the source model.\nBy deploying the Laplace transform and phasor arithmetic, closed-form\nexpressions of the source model spectrum can be derived. This facilitates\nthe calculation of the spectrum directly from the model parameters,\nwhich in turn makes it possible to obtain the ideal discrete spectrum\nof the model given the sampling frequency used. This discrete spectrum\nis entirely free of aliasing distortion, and the inverse discrete Fourier\ntransform is used to compute the sampled glottal flow pulse. The proposed\nmethod was applied to the widely used LF model, and the complete Laplace\ntransform of the model is presented. Also included are closed-form\nexpressions of the amplitude spectrum and the phase spectrum for the\ncalculation of the LF model spectrum.','https://www.isca-speech.org/archive/pdfs/interspeech_2021/gobl21_interspeech.pdf',12);
/*!40000 ALTER TABLE `ver0_paper` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `ver0_paper_affiliations`
--

DROP TABLE IF EXISTS `ver0_paper_affiliations`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `ver0_paper_affiliations` (
  `id` int NOT NULL AUTO_INCREMENT,
  `paper_id` int NOT NULL,
  `affiliation_id` int NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ver0_paper_affiliations_paper_id_affiliation_id_bc65ec33_uniq` (`paper_id`,`affiliation_id`),
  KEY `ver0_paper_affiliati_affiliation_id_1e0d8387_fk_ver0_affi` (`affiliation_id`),
  CONSTRAINT `ver0_paper_affiliati_affiliation_id_1e0d8387_fk_ver0_affi` FOREIGN KEY (`affiliation_id`) REFERENCES `ver0_affiliation` (`id`),
  CONSTRAINT `ver0_paper_affiliations_paper_id_2bd1874d_fk_ver0_paper_id` FOREIGN KEY (`paper_id`) REFERENCES `ver0_paper` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=289 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `ver0_paper_affiliations`
--

LOCK TABLES `ver0_paper_affiliations` WRITE;
/*!40000 ALTER TABLE `ver0_paper_affiliations` DISABLE KEYS */;
INSERT INTO `ver0_paper_affiliations` VALUES (145,1,1),(146,1,2),(147,3,3),(148,4,4),(149,4,5),(150,4,6),(152,5,7),(151,5,8),(153,6,9),(154,6,10),(155,7,11),(156,7,12),(157,8,13),(160,9,14),(161,9,15),(158,9,16),(159,9,17),(162,10,18),(163,12,19),(164,12,20),(165,12,21),(166,13,22),(167,14,23),(168,15,24),(169,16,25),(170,17,26),(171,18,27),(172,18,28),(173,19,29),(174,20,30),(175,20,31),(176,21,32),(177,21,33),(178,22,34),(179,23,35),(180,24,36),(181,24,37),(182,25,38),(184,26,39),(183,26,40),(185,27,41),(186,28,3),(187,29,4),(188,31,42),(189,31,43),(190,32,44),(191,33,45),(192,33,46),(194,34,47),(193,34,48),(195,35,49),(196,36,50),(197,37,51),(198,37,52),(199,38,53),(200,38,54),(201,39,55),(202,40,56),(203,41,57),(204,41,58),(205,42,59),(208,43,46),(206,43,60),(207,43,61),(209,44,62),(210,46,34),(211,47,63),(213,49,52),(212,49,64),(214,50,65),(215,50,66),(216,50,67),(217,50,68),(218,52,69),(219,53,70),(221,54,71),(220,54,72),(222,55,73),(223,55,74),(224,55,75),(225,56,76),(226,57,77),(227,57,78),(228,58,79),(229,60,80),(230,60,81),(231,60,82),(232,61,83),(233,62,84),(234,63,3),(235,64,85),(236,65,86),(237,65,87),(238,66,88),(239,67,89),(240,67,90),(241,68,91),(243,69,44),(242,69,92),(244,70,93),(245,70,94),(246,71,28),(247,71,95),(248,72,88),(249,73,96),(250,74,97),(251,75,98),(252,75,99),(253,76,100),(254,77,101),(256,78,102),(257,78,103),(255,78,104),(258,79,105),(259,79,106),(260,80,107),(261,81,108),(262,82,109),(264,83,110),(265,83,111),(263,83,112),(266,84,113),(267,84,114),(268,85,115),(269,85,116),(270,86,117),(271,87,118),(272,88,119),(274,89,79),(273,89,120),(275,91,121),(276,91,122),(277,92,123),(278,94,3),(279,96,124),(280,96,125),(281,96,126),(283,97,127),(282,97,128),(284,98,129),(285,99,112),(286,99,130),(287,100,131),(288,100,132);
/*!40000 ALTER TABLE `ver0_paper_affiliations` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `ver0_paper_authors`
--

DROP TABLE IF EXISTS `ver0_paper_authors`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `ver0_paper_authors` (
  `id` int NOT NULL AUTO_INCREMENT,
  `paper_id` int NOT NULL,
  `author_id` int NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ver0_paper_authors_paper_id_author_id_0e35c932_uniq` (`paper_id`,`author_id`),
  KEY `ver0_paper_authors_author_id_dfd50346_fk_ver0_author_id` (`author_id`),
  CONSTRAINT `ver0_paper_authors_author_id_dfd50346_fk_ver0_author_id` FOREIGN KEY (`author_id`) REFERENCES `ver0_author` (`id`),
  CONSTRAINT `ver0_paper_authors_paper_id_1481da37_fk_ver0_paper_id` FOREIGN KEY (`paper_id`) REFERENCES `ver0_paper` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=693 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `ver0_paper_authors`
--

LOCK TABLES `ver0_paper_authors` WRITE;
/*!40000 ALTER TABLE `ver0_paper_authors` DISABLE KEYS */;
INSERT INTO `ver0_paper_authors` VALUES (347,1,1),(348,1,2),(349,1,3),(350,2,4),(351,2,5),(352,2,6),(356,3,7),(353,3,8),(354,3,9),(355,3,10),(357,4,11),(358,4,12),(359,5,13),(360,5,14),(361,5,15),(362,6,16),(363,6,17),(364,6,18),(365,7,19),(366,7,20),(367,7,21),(368,7,22),(369,7,23),(370,7,24),(371,8,25),(372,8,26),(373,9,27),(374,9,28),(375,9,29),(376,9,30),(378,10,31),(377,10,32),(379,11,33),(380,11,34),(381,11,35),(382,11,36),(383,11,37),(384,11,38),(386,12,39),(385,12,40),(387,13,41),(388,13,42),(389,13,43),(390,14,44),(391,14,45),(392,15,46),(393,15,47),(394,16,48),(395,16,49),(396,16,50),(397,16,51),(398,16,52),(399,16,53),(401,17,54),(402,17,55),(400,17,56),(403,18,57),(404,18,58),(405,18,59),(406,18,60),(407,18,61),(408,19,62),(409,19,63),(410,20,64),(411,20,65),(412,20,66),(413,20,67),(414,21,68),(415,21,69),(416,21,70),(417,21,71),(418,22,72),(421,23,10),(419,23,73),(420,23,74),(422,24,75),(423,24,76),(424,25,77),(425,25,78),(426,26,79),(427,26,80),(428,26,81),(429,26,82),(430,26,83),(431,26,84),(432,27,85),(433,27,86),(435,28,87),(434,28,88),(436,29,89),(437,29,90),(438,30,91),(439,30,92),(440,30,93),(441,30,94),(443,31,95),(442,31,96),(444,32,97),(445,32,98),(446,32,99),(447,33,100),(448,33,101),(449,34,102),(450,34,103),(451,35,104),(452,35,105),(456,36,23),(457,36,24),(453,36,106),(454,36,107),(455,36,108),(458,37,109),(459,37,110),(460,37,111),(461,37,112),(462,37,113),(463,38,114),(464,38,115),(465,38,116),(466,38,117),(467,38,118),(468,38,119),(469,39,120),(470,39,121),(471,40,122),(472,40,123),(473,41,124),(474,41,125),(476,42,126),(477,42,127),(475,42,128),(478,43,129),(479,44,130),(480,44,131),(481,44,132),(482,45,133),(483,46,72),(484,46,134),(488,47,135),(485,47,136),(486,47,137),(487,47,138),(489,48,139),(491,49,109),(492,49,110),(494,49,113),(490,49,140),(493,49,141),(495,50,142),(496,50,143),(497,50,144),(498,50,145),(499,50,146),(500,50,147),(501,51,148),(502,51,149),(503,52,150),(507,53,151),(504,53,152),(505,53,153),(506,53,154),(508,54,155),(509,54,156),(510,54,157),(511,54,158),(515,55,159),(512,55,160),(513,55,161),(514,55,162),(516,56,163),(517,56,164),(518,56,165),(519,56,166),(523,57,167),(520,57,168),(521,57,169),(522,57,170),(524,58,171),(525,58,172),(526,59,10),(527,59,173),(528,59,174),(529,59,175),(530,59,176),(533,60,10),(531,60,177),(532,60,178),(534,60,179),(535,61,180),(536,61,181),(537,61,182),(538,61,183),(539,61,184),(540,62,185),(541,62,186),(542,63,9),(543,63,10),(544,63,175),(545,63,187),(546,63,188),(548,64,189),(549,64,190),(550,64,191),(547,64,192),(551,65,193),(552,65,194),(553,65,195),(554,65,196),(555,66,197),(556,66,198),(558,67,199),(557,67,200),(559,68,201),(560,68,202),(561,68,203),(562,68,204),(563,68,205),(564,68,206),(565,68,207),(566,68,208),(567,68,209),(568,68,210),(569,68,211),(570,69,212),(571,69,213),(572,69,214),(573,69,215),(574,69,216),(575,69,217),(576,69,218),(577,69,219),(578,69,220),(579,69,221),(581,70,50),(582,70,222),(583,70,223),(580,70,224),(584,71,225),(585,71,226),(586,71,227),(587,72,228),(588,72,229),(589,72,230),(592,73,231),(590,73,232),(591,73,233),(593,74,234),(594,74,235),(595,74,236),(596,75,237),(597,75,238),(598,75,239),(599,76,240),(600,76,241),(601,76,242),(602,76,243),(603,76,244),(604,76,245),(605,76,246),(607,77,247),(606,77,248),(610,78,78),(608,78,249),(609,78,250),(611,79,251),(612,79,252),(613,79,253),(614,80,254),(615,80,255),(616,81,256),(617,81,257),(618,81,258),(619,81,259),(620,82,260),(621,82,261),(622,82,262),(623,83,263),(624,83,264),(625,83,265),(626,83,266),(627,83,267),(628,83,268),(629,83,269),(631,84,270),(632,84,271),(630,84,272),(633,85,273),(634,85,274),(635,85,275),(636,85,276),(637,85,277),(639,86,156),(640,86,278),(641,86,279),(638,86,280),(642,87,281),(643,87,282),(644,87,283),(646,88,284),(647,88,285),(648,88,286),(649,88,287),(645,88,288),(654,89,172),(650,89,289),(651,89,290),(652,89,291),(653,89,292),(656,90,293),(657,90,294),(658,90,295),(655,90,296),(659,91,297),(660,91,298),(661,91,299),(662,91,300),(663,92,301),(664,92,302),(665,92,303),(666,92,304),(667,92,305),(668,92,306),(669,93,307),(670,93,308),(671,93,309),(672,93,310),(674,94,10),(675,94,311),(673,94,312),(676,95,313),(677,96,24),(678,96,314),(679,96,315),(680,97,316),(681,97,317),(682,97,318),(683,97,319),(684,98,320),(691,99,221),(685,99,321),(686,99,322),(687,99,323),(688,99,324),(689,99,325),(690,99,326),(692,100,327);
/*!40000 ALTER TABLE `ver0_paper_authors` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `ver0_paper_keys`
--

DROP TABLE IF EXISTS `ver0_paper_keys`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `ver0_paper_keys` (
  `id` int NOT NULL AUTO_INCREMENT,
  `paper_id` int NOT NULL,
  `keyword_id` int NOT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `ver0_paper_keys_paper_id_keyword_id_35e49985_uniq` (`paper_id`,`keyword_id`),
  KEY `ver0_paper_keys_keyword_id_13433f27_fk_ver0_keyword_id` (`keyword_id`),
  CONSTRAINT `ver0_paper_keys_keyword_id_13433f27_fk_ver0_keyword_id` FOREIGN KEY (`keyword_id`) REFERENCES `ver0_keyword` (`id`),
  CONSTRAINT `ver0_paper_keys_paper_id_170938ae_fk_ver0_paper_id` FOREIGN KEY (`paper_id`) REFERENCES `ver0_paper` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=1571 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `ver0_paper_keys`
--

LOCK TABLES `ver0_paper_keys` WRITE;
/*!40000 ALTER TABLE `ver0_paper_keys` DISABLE KEYS */;
INSERT INTO `ver0_paper_keys` VALUES (786,1,1),(787,1,2),(788,1,3),(789,1,4),(790,2,5),(791,2,6),(792,2,7),(793,2,8),(794,2,9),(795,2,10),(796,2,11),(797,2,12),(798,2,13),(799,2,14),(800,3,15),(801,3,16),(802,3,17),(803,3,18),(804,3,19),(805,3,20),(807,4,21),(808,4,22),(809,4,23),(806,4,24),(814,5,25),(815,5,26),(816,5,27),(817,5,28),(818,5,29),(819,5,30),(820,5,31),(810,5,32),(811,5,33),(812,5,34),(813,5,35),(826,6,24),(821,6,36),(822,6,37),(823,6,38),(824,6,39),(825,6,40),(827,7,41),(828,7,42),(829,7,43),(830,8,44),(831,8,45),(832,8,46),(833,8,47),(834,8,48),(835,8,49),(836,8,50),(837,8,51),(838,9,2),(839,9,52),(840,9,53),(841,9,54),(842,10,55),(843,11,2),(845,11,24),(844,11,56),(850,11,57),(846,11,58),(847,11,59),(848,11,60),(849,11,61),(855,12,41),(856,12,42),(857,12,43),(858,12,62),(859,12,63),(851,12,64),(852,12,65),(853,12,66),(854,12,67),(867,13,7),(860,13,68),(861,13,69),(862,13,70),(863,13,71),(864,13,72),(865,13,73),(866,13,74),(868,13,75),(869,13,76),(870,13,77),(871,13,78),(872,13,79),(873,13,80),(874,13,81),(875,13,82),(876,13,83),(877,15,1),(878,15,84),(879,15,85),(880,15,86),(881,15,87),(882,16,2),(889,16,24),(883,16,88),(884,16,89),(885,16,90),(886,16,91),(887,16,92),(888,16,93),(895,17,41),(896,17,42),(897,17,43),(892,17,66),(898,17,94),(899,17,95),(890,17,96),(891,17,97),(893,17,98),(894,17,99),(900,18,100),(901,18,101),(902,18,102),(903,18,103),(904,18,104),(905,18,105),(906,18,106),(907,18,107),(908,19,108),(909,19,109),(910,19,110),(911,19,111),(912,19,112),(914,20,7),(913,20,38),(915,20,113),(916,20,114),(917,20,115),(918,20,116),(919,21,117),(920,21,118),(921,21,119),(922,21,120),(923,21,121),(924,21,122),(927,22,117),(928,22,123),(929,22,124),(930,22,125),(931,22,126),(932,22,127),(925,22,128),(926,22,129),(942,23,87),(933,23,130),(934,23,131),(935,23,132),(936,23,133),(937,23,134),(938,23,135),(939,23,136),(940,23,137),(941,23,138),(943,24,70),(944,24,139),(945,24,140),(946,24,141),(947,24,142),(948,24,143),(949,24,144),(950,24,145),(951,25,146),(952,25,147),(953,25,148),(954,25,149),(955,25,150),(956,25,151),(957,26,2),(958,26,152),(959,26,153),(960,26,154),(961,26,155),(962,26,156),(963,26,157),(967,27,15),(965,27,17),(966,27,158),(964,27,159),(972,28,19),(968,28,160),(969,28,161),(970,28,162),(971,28,163),(973,29,129),(974,29,164),(975,29,165),(976,29,166),(977,30,38),(978,30,167),(979,30,168),(980,30,169),(981,30,170),(982,30,171),(983,30,172),(984,30,173),(986,31,174),(987,31,175),(985,31,176),(988,32,1),(989,32,36),(996,32,55),(994,32,117),(990,32,177),(991,32,178),(992,32,179),(993,32,180),(995,32,181),(997,32,182),(998,32,183),(999,33,184),(1000,33,185),(1001,33,186),(1002,33,187),(1011,34,72),(1013,34,143),(1014,34,188),(1015,34,189),(1016,34,190),(1017,34,191),(1003,34,192),(1004,34,193),(1005,34,194),(1006,34,195),(1007,34,196),(1008,34,197),(1009,34,198),(1010,34,199),(1012,34,200),(1021,35,22),(1022,35,24),(1018,35,136),(1019,35,201),(1020,35,202),(1032,36,58),(1023,36,203),(1024,36,204),(1025,36,205),(1026,36,206),(1027,36,207),(1028,36,208),(1029,36,209),(1030,36,210),(1031,36,211),(1033,37,212),(1034,37,213),(1035,37,214),(1036,37,215),(1037,37,216),(1038,37,217),(1039,37,218),(1040,37,219),(1041,37,220),(1042,37,221),(1043,37,222),(1044,37,223),(1045,38,224),(1046,38,225),(1047,38,226),(1048,38,227),(1049,38,228),(1050,38,229),(1051,38,230),(1059,39,40),(1052,39,231),(1053,39,232),(1054,39,233),(1055,39,234),(1056,39,235),(1057,39,236),(1058,39,237),(1060,39,238),(1061,40,70),(1069,40,89),(1062,40,239),(1063,40,240),(1064,40,241),(1065,40,242),(1066,40,243),(1067,40,244),(1068,40,245),(1070,41,1),(1071,41,72),(1078,41,92),(1072,41,143),(1073,41,246),(1074,41,247),(1075,41,248),(1076,41,249),(1077,41,250),(1079,42,114),(1080,42,251),(1081,42,252),(1082,42,253),(1089,43,9),(1085,43,193),(1088,43,198),(1090,43,254),(1091,43,255),(1083,43,256),(1084,43,257),(1086,43,258),(1087,43,259),(1100,45,186),(1092,45,260),(1093,45,261),(1094,45,262),(1095,45,263),(1096,45,264),(1097,45,265),(1098,45,266),(1099,45,267),(1101,46,1),(1102,46,268),(1103,46,269),(1104,46,270),(1105,46,271),(1106,46,272),(1107,47,226),(1108,47,273),(1109,47,274),(1110,47,275),(1111,47,276),(1112,47,277),(1113,47,278),(1114,47,279),(1115,47,280),(1116,47,281),(1117,47,282),(1118,47,283),(1119,47,284),(1121,49,214),(1122,49,216),(1123,49,285),(1124,49,286),(1125,49,287),(1120,49,288),(1134,50,40),(1136,50,92),(1135,50,186),(1130,50,261),(1126,50,289),(1127,50,290),(1128,50,291),(1129,50,292),(1131,50,293),(1132,50,294),(1133,50,295),(1144,51,89),(1137,51,296),(1138,51,297),(1139,51,298),(1140,51,299),(1141,51,300),(1142,51,301),(1143,51,302),(1151,52,151),(1150,52,244),(1145,52,303),(1146,52,304),(1147,52,305),(1148,52,306),(1149,52,307),(1152,53,2),(1160,53,24),(1162,53,92),(1155,53,143),(1161,53,186),(1154,53,295),(1156,53,308),(1157,53,309),(1158,53,310),(1159,53,311),(1153,53,312),(1164,54,2),(1165,54,15),(1166,54,17),(1163,54,64),(1167,54,313),(1168,54,314),(1169,54,315),(1170,54,316),(1171,54,317),(1172,54,318),(1180,55,22),(1181,55,24),(1177,55,38),(1179,55,49),(1182,55,186),(1178,55,295),(1183,55,319),(1173,55,320),(1174,55,321),(1175,55,322),(1176,55,323),(1184,56,96),(1185,56,324),(1186,56,325),(1187,56,326),(1195,57,24),(1196,57,253),(1188,57,292),(1189,57,327),(1190,57,328),(1191,57,329),(1192,57,330),(1193,57,331),(1194,57,332),(1197,58,2),(1204,58,24),(1198,58,333),(1199,58,334),(1200,58,335),(1201,58,336),(1202,58,337),(1203,58,338),(1205,59,19),(1213,60,19),(1214,60,20),(1206,60,204),(1207,60,206),(1208,60,207),(1209,60,339),(1210,60,340),(1211,60,341),(1212,60,342),(1215,60,343),(1218,61,42),(1219,61,43),(1226,61,62),(1220,61,344),(1221,61,345),(1222,61,346),(1223,61,347),(1224,61,348),(1225,61,349),(1217,61,350),(1227,61,351),(1216,61,352),(1228,62,353),(1229,62,354),(1230,62,355),(1231,62,356),(1232,62,357),(1233,62,358),(1234,62,359),(1235,62,360),(1236,62,361),(1240,63,15),(1237,63,17),(1239,63,19),(1238,63,362),(1241,64,226),(1246,64,245),(1242,64,363),(1243,64,364),(1244,64,365),(1245,64,366),(1247,65,227),(1248,65,367),(1249,65,368),(1250,65,369),(1251,65,370),(1252,65,371),(1259,66,28),(1257,66,87),(1253,66,199),(1254,66,372),(1255,66,373),(1256,66,374),(1260,66,375),(1258,66,376),(1263,67,377),(1264,67,378),(1265,67,379),(1266,67,380),(1267,67,381),(1268,67,382),(1269,67,383),(1261,67,384),(1262,67,385),(1274,68,2),(1278,68,24),(1270,68,386),(1271,68,387),(1272,68,388),(1273,68,389),(1275,68,390),(1276,68,391),(1277,68,392),(1279,69,2),(1288,69,24),(1290,69,42),(1291,69,43),(1295,69,58),(1296,69,72),(1298,69,92),(1289,69,168),(1297,69,210),(1292,69,308),(1293,69,309),(1294,69,310),(1280,69,393),(1281,69,394),(1282,69,395),(1283,69,396),(1284,69,397),(1285,69,398),(1286,69,399),(1287,69,400),(1300,70,40),(1299,70,96),(1301,70,334),(1302,70,401),(1303,70,402),(1304,70,403),(1305,70,404),(1306,70,405),(1307,70,406),(1308,70,407),(1309,70,408),(1310,71,226),(1311,71,409),(1312,71,410),(1313,71,411),(1314,71,412),(1318,72,24),(1316,72,198),(1317,72,328),(1319,72,413),(1320,72,414),(1321,72,415),(1315,72,416),(1322,73,1),(1323,73,2),(1330,73,24),(1329,73,245),(1325,73,292),(1328,73,365),(1324,73,417),(1326,73,418),(1327,73,419),(1338,75,58),(1337,75,295),(1339,75,351),(1331,75,420),(1332,75,421),(1333,75,422),(1334,75,423),(1335,75,424),(1336,75,425),(1344,76,367),(1340,76,426),(1341,76,427),(1342,76,428),(1343,76,429),(1345,77,2),(1346,77,72),(1347,77,430),(1348,77,431),(1349,77,432),(1350,78,297),(1351,78,433),(1352,78,434),(1353,78,435),(1354,78,436),(1355,78,437),(1356,78,438),(1357,78,439),(1358,78,440),(1359,79,441),(1360,79,442),(1361,79,443),(1362,79,444),(1363,79,445),(1372,80,20),(1369,80,198),(1373,80,378),(1374,80,379),(1366,80,382),(1370,80,383),(1371,80,427),(1375,80,446),(1376,80,447),(1364,80,448),(1365,80,449),(1367,80,450),(1368,80,451),(1377,81,2),(1378,81,452),(1379,81,453),(1380,81,454),(1381,81,455),(1382,81,456),(1383,81,457),(1384,81,458),(1385,81,459),(1386,82,1),(1395,82,55),(1390,82,72),(1388,82,101),(1396,82,186),(1387,82,261),(1389,82,295),(1391,82,460),(1392,82,461),(1393,82,462),(1394,82,463),(1398,83,72),(1399,83,108),(1404,83,186),(1397,83,261),(1403,83,371),(1400,83,464),(1401,83,465),(1402,83,466),(1407,84,41),(1421,84,62),(1406,84,317),(1409,84,334),(1418,84,347),(1420,84,349),(1410,84,467),(1411,84,468),(1412,84,469),(1413,84,470),(1414,84,471),(1415,84,472),(1416,84,473),(1417,84,474),(1419,84,475),(1408,84,476),(1405,84,477),(1426,85,24),(1425,85,227),(1424,85,355),(1427,85,478),(1428,85,479),(1422,85,480),(1423,85,481),(1436,86,55),(1437,86,186),(1430,86,226),(1432,86,261),(1429,86,482),(1431,86,483),(1433,86,484),(1434,86,485),(1435,86,486),(1451,87,30),(1442,87,72),(1450,87,92),(1444,87,204),(1447,87,206),(1448,87,207),(1438,87,294),(1449,87,337),(1439,87,487),(1440,87,488),(1441,87,489),(1443,87,490),(1445,87,491),(1446,87,492),(1452,88,493),(1453,88,494),(1454,88,495),(1456,89,2),(1455,89,129),(1457,89,496),(1458,89,497),(1459,89,498),(1460,89,499),(1461,89,500),(1462,89,501),(1463,90,101),(1464,90,457),(1465,90,493),(1466,90,502),(1467,90,503),(1468,90,504),(1469,90,505),(1470,90,506),(1471,90,507),(1472,90,508),(1473,90,509),(1474,90,510),(1475,90,511),(1478,91,2),(1486,91,24),(1484,91,72),(1485,91,493),(1476,91,512),(1477,91,513),(1479,91,514),(1480,91,515),(1481,91,516),(1482,91,517),(1483,91,518),(1487,92,2),(1496,92,24),(1494,92,245),(1495,92,311),(1493,92,365),(1488,92,519),(1489,92,520),(1490,92,521),(1491,92,522),(1492,92,523),(1497,93,138),(1498,93,524),(1499,93,525),(1500,93,526),(1501,93,527),(1502,93,528),(1506,94,15),(1510,94,17),(1514,94,19),(1503,94,161),(1505,94,206),(1511,94,341),(1507,94,529),(1508,94,530),(1509,94,531),(1512,94,532),(1513,94,533),(1515,94,534),(1504,94,535),(1519,96,24),(1517,96,55),(1518,96,536),(1516,96,537),(1520,96,538),(1521,96,539),(1522,96,540),(1524,97,193),(1527,97,198),(1531,97,282),(1534,97,287),(1526,97,288),(1530,97,429),(1528,97,457),(1532,97,541),(1533,97,542),(1529,97,543),(1523,97,544),(1525,97,545),(1540,98,7),(1542,98,496),(1535,98,546),(1536,98,547),(1537,98,548),(1538,98,549),(1539,98,550),(1541,98,551),(1561,99,245),(1560,99,365),(1546,99,388),(1543,99,512),(1544,99,513),(1545,99,515),(1547,99,552),(1548,99,553),(1549,99,554),(1550,99,555),(1551,99,556),(1552,99,557),(1553,99,558),(1554,99,559),(1555,99,560),(1556,99,561),(1557,99,562),(1558,99,563),(1559,99,564),(1562,100,334),(1563,100,565),(1564,100,566),(1565,100,567),(1566,100,568),(1567,100,569),(1568,100,570),(1569,100,571),(1570,100,572);
/*!40000 ALTER TABLE `ver0_paper_keys` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2022-02-24 14:20:43
