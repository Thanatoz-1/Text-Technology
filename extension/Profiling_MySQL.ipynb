{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad407f6a-76a9-4fdf-9415-04d2e1fb654e",
   "metadata": {},
   "source": [
    "# MySQL profiling \n",
    "This notebook will first load actual papers from a formatted XML, then start profiling with these data on MySQL database.\n",
    "Each row in the table represents one paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536d1f5-3515-412b-8b7b-427e8c78ef39",
   "metadata": {},
   "source": [
    "## Load data from XML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26f1cce",
   "metadata": {
    "id": "f26f1cce"
   },
   "outputs": [],
   "source": [
    "# by yixuan\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import bs4, lxml\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754c265",
   "metadata": {
    "id": "1754c265"
   },
   "outputs": [],
   "source": [
    "def get_timestamp():\n",
    "    return f'{time.time()}'\n",
    "\n",
    "def generate_ids(num, total):\n",
    "    randomlist = []\n",
    "    for i in range(num):\n",
    "        randomlist.append(random.randint(0, total-1))\n",
    "    return randomlist\n",
    "\n",
    "class XMLLoader:\n",
    "    \"\"\"\n",
    "    Load XML files into bs4 content\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.xmls = []\n",
    "        self.items = []\n",
    "    \n",
    "    def read_xmls(self, xmls_path):\n",
    "        \"\"\"\n",
    "        xmls_path contains all the xmls files's paths\n",
    "        one line for one xml path\n",
    "        each xml file will be parsed and stored as a BeautifulSoup\n",
    "        parsed object in self.xmls\n",
    "        \"\"\"\n",
    "        print('loading...')\n",
    "        with open(xmls_path, 'r') as f:\n",
    "            for xml_path in f:\n",
    "                self.xmls.append(self._read_one_xml(xml_path))\n",
    "                \n",
    "        # serialize items, just load it to the memory\n",
    "        for bs_content in tqdm(self.xmls):\n",
    "            for item in bs_content.find_all('item'):\n",
    "                title = item.title.get_text()\n",
    "                abstract = item.abstract.get_text()\n",
    "                author = item.author.get_text()\n",
    "                url = item.url.get_text()\n",
    "                self.items.append((title, abstract, author, url))\n",
    "                \n",
    "    \n",
    "    def _read_one_xml(self, xml_path):\n",
    "        \"\"\"\n",
    "        return the BeautifulSoup parsed result of a given xml file\n",
    "        \"\"\"\n",
    "        xml_path = xml_path.strip()\n",
    "        with open(xml_path, 'r') as f:\n",
    "            content = f.read()\n",
    "            bs_content = bs(content, 'lxml')\n",
    "            return bs_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d2b2d",
   "metadata": {
    "id": "a05d2b2d"
   },
   "outputs": [],
   "source": [
    "xml_loader = XMLLoader()\n",
    "xml_loader.read_xmls('xml_paths')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5255a53d-3153-44cb-9c25-3defe47432c3",
   "metadata": {},
   "source": [
    "## MySQL profiling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93590697-f6af-4d37-a0c3-3ae03191f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following are the data structures useful for handling with DB\n",
    "class DatabaseHandler:\n",
    "    def __init__(self, items):\n",
    "        self.items = items\n",
    "        \n",
    "    def connect(self):\n",
    "        \"\"\"\n",
    "        connect to a db, return the connect result\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    def load_to_db(self, num=1000):\n",
    "        \"\"\"\n",
    "        insertion test\n",
    "        randomly sample num xml items and upload them \n",
    "        to db along with the timestamp, return the total \n",
    "        time usage \n",
    "        \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "class MySQL(DatabaseHandler):\n",
    "    def __init__(self, items, host, user, password, db_name):\n",
    "        super().__init__(items)\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.db_name = db_name\n",
    "        self.db = None\n",
    "    \n",
    "    def _print_cursor(self, cursor):\n",
    "        for x in cursor:\n",
    "            print(x)\n",
    "        \n",
    "    def connect(self):\n",
    "        self.db = mysql.connector.connect(\n",
    "            host = self.host, \n",
    "            user = self.user, \n",
    "            password = self.password, \n",
    "            database = self.db_name\n",
    "        )\n",
    "        print(self.db)\n",
    "    \n",
    "    def create_table(self, table_name):\n",
    "        \"\"\"\n",
    "        create a test table\n",
    "        \"\"\"\n",
    "        \n",
    "        self.table_name = table_name\n",
    "        sql = f'''CREATE TABLE {self.table_name} (\n",
    "            title VARCHAR(255) NOT NULL,\n",
    "            abstract VARCHAR(4096) NOT NULL, \n",
    "            author VARCHAR(512) NOT NULL, \n",
    "            url VARCHAR(512) NOT NULL, \n",
    "            timestamp VARCHAR(64) NOT NULL, \n",
    "            id INT PRIMARY KEY AUTO_INCREMENT)\n",
    "            '''\n",
    "        cursor = self.db.cursor()\n",
    "        cursor.execute(f'DROP TABLE IF EXISTS {self.table_name}')\n",
    "        self._print_cursor(cursor)\n",
    "        cursor.execute(sql)\n",
    "        self._print_cursor(cursor)\n",
    "    \n",
    "    \n",
    "    def show_tables(self):\n",
    "        cursor = self.db.cursor()\n",
    "        cursor.execute('SHOW TABLES')\n",
    "        self._print_cursor(cursor)\n",
    "    \n",
    "    def show_k_rows(self, table, k=5):\n",
    "        cursor = self.db.cursor()\n",
    "        cursor.execute(f'SELECT * FROM {table} LIMIT {k}')\n",
    "        self._print_cursor(cursor)\n",
    "        \n",
    "    def load_to_db(self, num, table):\n",
    "        total = len(self.items)\n",
    "        randomlist = generate_ids(num, total)\n",
    "        cursor = self.db.cursor()\n",
    "        for i in tqdm(randomlist):\n",
    "            sql = f'''INSERT INTO {table}(\n",
    "                title, abstract, author, url, timestamp) \n",
    "                VALUES (%s, %s, %s, %s, %s)'''\n",
    "            basic_item = self.items[i]\n",
    "            basic_item += (get_timestamp(), )\n",
    "            cursor.execute(sql, basic_item)\n",
    "            self.db.commit()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe12cf",
   "metadata": {
    "id": "5cbe12cf"
   },
   "outputs": [],
   "source": [
    "# connect to db and create an empty table named 'small test'\n",
    "table = 'small_test'\n",
    "mysql_loader = MySQL(xml_loader.items, 'localhost', 'ttt', 'password@ttt', 'ttt_db_efficiency')\n",
    "mysql_loader.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2967cd2",
   "metadata": {
    "id": "a2967cd2",
    "outputId": "8757980e-7767-49f3-a70d-4010fce9358a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('papers',)\n",
      "('small_test',)\n"
     ]
    }
   ],
   "source": [
    "mysql_loader.create_table(table)\n",
    "mysql_loader.show_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf4235-3749-4415-8091-8cd302a46d4f",
   "metadata": {},
   "source": [
    "### Writing Test\n",
    "write 1m papers to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adad1e7-3ba4-4615-b348-5cc5ca762607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 1m papers and load them to the db\n",
    "mysql_loader.load_to_db(1000000, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e92ef0c",
   "metadata": {
    "id": "9e92ef0c",
    "outputId": "9eb6472b-dfff-4719-afc8-8199f0488a75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RSR2015: database for text-dependent speaker verification using multiple pass-phrases', 'This paper describes a new speech corpus, the RSR2015 database designed for text-dependent speaker recognition with scenario based on fixed pass-phrases. This database consists of over 71 hours of speech recorded from English speakers covering the diversity of accents spoken in Singapore. Acquisition has been done using a set of six portable devices including smart phones and tablets. The pool of speakers consists of 298 participants (142 female and 156 male speakers) from 17 to 42 years old. We propose a protocol for the case of user-dependent passphrases in text-dependent speaker recognition and we also report speaker recognition experiments on RSR2015 database.\\n', 'Anthony Larcher, Kong Aik Lee, Bin Ma, Haizhou Li', 'https://www.isca-speech.org/archive/pdfs/interspeech_2012/larcher12_interspeech.pdf', '1640363508.2227705', 1)\n",
      "('Phonological Markers of Oxytocin and MDMA Ingestion', 'Speech data has the potential to become a powerful tool to provide\\nquantitative information about emotion beyond that achieved by subjective\\nassessments. Based on this concept, we investigate the use of speech\\nto identify effects in subjects under the influence of two different\\ndrugs: Oxytocin (OT) and 3,4-methylenedioxymethamphetamine (MDMA),\\nalso known as ecstasy. We extract a set of informative phonological\\nfeatures that can characterize emotion. Then, we perform classification\\nto detect if the subject is under the influence of a drug. Our best\\nresults show low error rates of 13% and 17% for the subject classification\\nof OT and MDMA vs. placebo, respectively. We also analyze the performance\\nof the features to differentiate the two levels of MDMA doses, obtaining\\nan error rate of 19%. The results indicate that subtle emotional changes\\ncan be detected in the context of drug use.\\n', 'Carla Agurto, Raquel Norel, Rachel Ostrand, Gillinder Bedi, Harriet de Wit, Matthew J. Baggott, Matthew G. Kirkpatrick, Margaret Wardle, Guillermo A. Cecchi', 'https://www.isca-speech.org/archive/pdfs/interspeech_2017/agurto17_interspeech.pdf', '1640363508.247244', 2)\n",
      "('Blind source separation using spatially distributed microphones based on microphone-location dependent source activities', 'Distributed microphone array (DMA) processing has recently been gathering increasing research interest due to its various applications and diverse challenges. In many conventional multi-channel speech enhancement algorithms that use co-located microphones, such as the multi-channel Wiener filtering and mask-based blind source separation (BSS) approaches, statistics of the target and interference signals are required if we are to design an optimal enhancement filter. To obtain such statistics, we estimate activity information regarding source and interference signals (hereafter, source activity information), that is generally assumed to be common to all the microphones. However, in DMA scenarios, the source activities observable at any given microphone may be significantly different from those of others when the microphones are spatially distributed to a great degree, and the level of each signal at each microphone varies significantly. Thus, to capture such source activity information appropriately and thereby achieve optimal speech enhancement in DMA environments, in this paper we propose an approach for estimating microphone-dependent source activity, and for performing blind source separation based on such information. The proposed method estimates the activity of each source signal at each microphone, which can be explained by the microphone-independent speech log power spectra and microphone-location dependent source gains. We introduce a probabilistic formulation of the proposed method, and an efficient algorithm for model parameter estimation. We show the efficacy of the proposed method experimentally in comparison with conventional methods in various DMA scenarios.\\n', 'Keisuke Kinoshita, Mehrez Souden, Tomohiro Nakatani', 'https://www.isca-speech.org/archive/pdfs/interspeech_2013/kinoshita13_interspeech.pdf', '1640363508.2674003', 3)\n",
      "('An anisotropic diffusion filter based on multidirectional separability', 'Extracting tongue contour from high noised ultrasound image is a key issue of observing speech production procedure. Anisotropic diffusion has been widely used in reducing speckle noise of ultrasound images but it is not very effective in preserving edges and tends to blur them. Hence the blurred edges hamper the succeeding contour-based pattern analysis or modeling. In this study, we modify the standard SRAD (speckle reducing anisotropic diffusion) to improve its edge detection and suppress the intrinsic edge blurring effect of SRAD by exploiting the multidirectional separability. We experimented with both synthetic and real ultrasound images by SRAD and the proposed approach. The extracted contours in denoised images by SRAD and the proposed approach are compared in terms of the corresponding accuracy, both subjectively and objectively. The results show the proposed approach performs better than the conventional SRAD and more accurate contours can be obtained for post processing.\\n', 'Shen Liu, Jianguo Wei, Xin Wang, Wenhuan Lu, Qiang Fang, Jianwu Dang', 'https://www.isca-speech.org/archive/pdfs/interspeech_2013/liu13h_interspeech.pdf', '1640363508.276732', 4)\n",
      "('Speaker-specific Structure in German Voiceless Stop Voice Onset Times', 'Voice onset time (VOT), a primary cue for voicing in many languages including English and German, is known to vary greatly between speakers, but also displays robust within-speaker consistencies, at least in English. The current analysis extends these findings to German. VOT measures were investigated from voiceless alveolar and velar stops in CV syllables cued by a visual prompt in a cue-distractor task. Comparably to English, a considerable portion of German VOT variability can be attributed to the syllable’s vowel length and the stop’s place of articulation. Individual differences in VOT still remain irrespective of speech rate. However, significant correlations across places of articulation and between speaker-specific mean VOTs and standard deviations indicate that talkers employ a relatively unified VOT profile across places of articulation. This could allow listeners to more efficiently adapt to speaker-specific realisations. ', 'Marc Antony Hullebus, Stephen Tobin, Adamantios Gafos', 'https://www.isca-speech.org/archive/pdfs/interspeech_2018/hullebus18_interspeech.pdf', '1640363508.2835166', 5)\n"
     ]
    }
   ],
   "source": [
    "# show the first k results\n",
    "mysql_loader.show_k_rows(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c71c47-ec72-4ad0-b59d-ad244cc236a4",
   "metadata": {},
   "source": [
    "### Query Test\n",
    "- Firstly query whether a data point exists, e.g., whether a title exists;\n",
    "- Secondly query whether a word exists in the content of a data point, e.g. whether a keyword exists in the abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4693505e",
   "metadata": {
    "id": "4693505e"
   },
   "outputs": [],
   "source": [
    "# query test\n",
    "\n",
    "class Querier:\n",
    "    def __init__(self, items, db):\n",
    "        self.items = items\n",
    "        self.db = db\n",
    "        self.vocab = []\n",
    "        self.name = None\n",
    "        self.idx = None\n",
    "    \n",
    "    def prepare_query_list(self, idx):\n",
    "        # item structure\n",
    "        # self.items.append((title, abstract, author, url))\n",
    "        vocab = set()\n",
    "        for line in self.items:\n",
    "            line = line[idx].split()\n",
    "            vocab = vocab.union(set(line))\n",
    "        self.vocab = list(vocab)\n",
    "    \n",
    "    def single_query(self, num, table):\n",
    "        \"\"\"\n",
    "        whether a tag exist\n",
    "        \"\"\"\n",
    "        total = len(self.items)\n",
    "        randomlist = generate_ids(num, total)\n",
    "        elapsed = 0\n",
    "        cursor = self.db.cursor()\n",
    "        for i in randomlist:\n",
    "            sql = f'SELECT * FROM {table} WHERE {self.name} = %s'\n",
    "            tic = time.perf_counter()\n",
    "            cursor.execute(sql, (self.items[i][self.idx],))\n",
    "            cursor.fetchall()\n",
    "            toc = time.perf_counter()\n",
    "            elapsed += toc-tic\n",
    "        return elapsed\n",
    "    \n",
    "    def keyword_query(self, num, table):\n",
    "        \"\"\"\n",
    "        whether a keyword exists in the content\n",
    "        \"\"\"\n",
    "        total = len(self.items)\n",
    "        randomlist = generate_ids(num, total)\n",
    "        elapsed = 0\n",
    "        cursor = self.db.cursor()\n",
    "        for i in randomlist:\n",
    "            sql = f'SELECT * FROM {table} WHERE {self.name} LIKE %s'\n",
    "            tic = time.perf_counter()\n",
    "            cursor.execute(sql, ('%'+self.vocab[i]+'%',))\n",
    "            cursor.fetchall()\n",
    "            toc = time.perf_counter()\n",
    "            elapsed += toc-tic\n",
    "        return elapsed\n",
    "    \n",
    "    \n",
    "class TitleQuerier(Querier):\n",
    "    def __init__(self, items, db):\n",
    "        super().__init__(items, db)\n",
    "        self.idx = 0\n",
    "        self.name = 'title'\n",
    "        self.prepare_query_list(self.idx)\n",
    "        \n",
    "class AbstractQuerier(Querier):\n",
    "    def __init__(self, items, db):\n",
    "        super().__init__(items, db)\n",
    "        self.idx = 1\n",
    "        self.name = 'abstract'\n",
    "        self.prepare_query_list(self.idx)\n",
    "        \n",
    "class AuthorQuerier(Querier):\n",
    "    def __init__(self, items, db):\n",
    "        super().__init__(items, db)\n",
    "        self.idx = 2\n",
    "        self.name = 'url'\n",
    "        self.prepare_query_list(self.idx)\n",
    "        \n",
    "class UrlQuerier(Querier):\n",
    "    def __init__(self, items, db):\n",
    "        super().__init__(items, db)\n",
    "        self.idx = 0\n",
    "        self.name = 'url'\n",
    "        self.prepare_query_list(self.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6d313",
   "metadata": {
    "id": "eae6d313"
   },
   "outputs": [],
   "source": [
    "title_query = TitleQuerier(mysql_loader.items, mysql_loader.db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f163b79",
   "metadata": {
    "id": "0f163b79"
   },
   "outputs": [],
   "source": [
    "def profile_query(test_count, query_func, num_per_time, table):\n",
    "    test_time = 0\n",
    "    for i in tqdm(range(test_count)):\n",
    "        test_time += query_func(num_per_time, table)\n",
    "    print(test_time/test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765fff08",
   "metadata": {
    "id": "765fff08",
    "outputId": "c196fd8f-2d21-4bb9-c897-c6393cb6b6bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:05<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1363654257263989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_of_test = 5\n",
    "query_per_test = 1000\n",
    "profile_query(num_of_test, title_query.single_query, query_per_test, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b35a0b",
   "metadata": {
    "id": "41b35a0b",
    "outputId": "aba9c620-58e2-46f0-83d0-b9c49ff7f745"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 5/5 [00:10<00:00,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0957737660501152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_of_test = 5\n",
    "query_per_test = 1000\n",
    "profile_query(num_of_test, title_query.keyword_query, query_per_test, table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Profiling_MySQL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
