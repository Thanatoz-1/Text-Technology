{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"topic modelling.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1eRRB4iGD4M_kVeVCLtJxWgd7lACtXc-V","authorship_tag":"ABX9TyN4nFcHNFwgVLZx68o2kufY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"72kc41UguOH0"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import re, nltk, spacy, gensim\n","\n","# Sklearn\n","from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.model_selection import GridSearchCV\n","from pprint import pprint\n","\n","import xml.etree.ElementTree as ET \n","\n","# Plotting tools\n","# import pyLDAvis\n","# import pyLDAvis.sklearn\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n"]},{"cell_type":"code","source":["xmlpath = '/content/drive/MyDrive/2021/stuttgart/Text Tech Team/resources/interspeech/all_formatted.xml'\n","tree = ET.parse(xmlpath)\n","root = tree.getroot() "],"metadata":{"id":"8JzswHCOwcwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus = []\n","for conf in root:\n","    meta = conf[0]\n","    papers = conf[1]\n","    for paper in papers:\n","        corpus.append(paper[1].text)"],"metadata":{"id":"5a6F7aO3xSIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n","\n","words = list(sent_to_words(corpus))"],"metadata":{"id":"KknoyH_qx_Cf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp = spacy.load('en', disable=['parser', 'ner'])\n","def lemmatisation(words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    ret = []\n","    for sent in words:\n","        doc = nlp(\" \".join(sent)) \n","        ret.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n","    return ret\n","\n","# Only use open class data\n","lemmas = lemmatisation(words)"],"metadata":{"id":"ccDai8JnyLi6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count_vectorizer = CountVectorizer(analyzer='word',       \n","                             min_df=10,                        \n","                             stop_words='english',             \n","                             lowercase=True,                   \n","                             token_pattern='[a-zA-Z0-9]{3,}')\n","\n","word_vecs = count_vectorizer.fit_transform(lemmas)\n","\n","\n","lda_model = LatentDirichletAllocation(n_components=50,               \n","                                      max_iter=20,               \n","                                      learning_method='online',   \n","                                      random_state=771,          \n","                                      batch_size=128,            \n","                                      evaluate_every = -1,       \n","                                      n_jobs = -1)\n","lda_output = lda_model.fit_transform(word_vecs)"],"metadata":{"id":"UZy6N8H7yjTi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# show top k keywords for n topics\n","def show_topk(vectorizer, lda, topk=10, n=20):\n","    keys = np.array(vectorizer.get_feature_names())\n","    topics = []\n","    for weights in lda_model.components_:\n","        keys_idx = (-weights).argsort()[:topk]\n","        topics.append(keys.take(keys_idx))\n","        if len(topics) == n:\n","            break\n","    return topics \n","\n","topics = show_topk(count_vectorizer, lda_model)        \n","\n","fig = pd.DataFrame(topics)\n","fig.columns = ['Word '+str(i) for i in range(fig.shape[1])]\n","fig.index = ['Topic '+str(i) for i in range(fig.shape[0])]\n","fig"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":733},"id":"mQw5TYv-zjag","executionInfo":{"status":"ok","timestamp":1643038226119,"user_tz":-60,"elapsed":242,"user":{"displayName":"Easion","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GidVM3Zwzvv68A9mBhDCxuMOh3et_teKEg0BSwx=s64","userId":"12241261983555706169"}},"outputId":"7cf8fead-53a2-4cdf-af1f-646d6f67f732"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n","  warnings.warn(msg, category=FutureWarning)\n"]},{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-a698a09c-3e62-4a47-b8ed-34881bb82bb1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Word 0</th>\n","      <th>Word 1</th>\n","      <th>Word 2</th>\n","      <th>Word 3</th>\n","      <th>Word 4</th>\n","      <th>Word 5</th>\n","      <th>Word 6</th>\n","      <th>Word 7</th>\n","      <th>Word 8</th>\n","      <th>Word 9</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Topic 0</th>\n","      <td>angular</td>\n","      <td>inconsistency</td>\n","      <td>deceptive</td>\n","      <td>pruning</td>\n","      <td>articulation</td>\n","      <td>squeeze</td>\n","      <td>empirically</td>\n","      <td>encode</td>\n","      <td>disfluency</td>\n","      <td>concurrent</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 1</th>\n","      <td>unit</td>\n","      <td>segment</td>\n","      <td>acoustic</td>\n","      <td>event</td>\n","      <td>infant</td>\n","      <td>game</td>\n","      <td>audiovisual</td>\n","      <td>linguistic</td>\n","      <td>forensic</td>\n","      <td>discover</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 2</th>\n","      <td>human</td>\n","      <td>user</td>\n","      <td>conversation</td>\n","      <td>turn</td>\n","      <td>dialog</td>\n","      <td>interaction</td>\n","      <td>conversational</td>\n","      <td>spoof</td>\n","      <td>automate</td>\n","      <td>switching</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 3</th>\n","      <td>vowel</td>\n","      <td>perception</td>\n","      <td>cue</td>\n","      <td>study</td>\n","      <td>participant</td>\n","      <td>formant</td>\n","      <td>perceptual</td>\n","      <td>effect</td>\n","      <td>stimulus</td>\n","      <td>experiment</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 4</th>\n","      <td>datum</td>\n","      <td>use</td>\n","      <td>text</td>\n","      <td>language</td>\n","      <td>task</td>\n","      <td>dialogue</td>\n","      <td>speech</td>\n","      <td>automatic</td>\n","      <td>transcription</td>\n","      <td>paper</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 5</th>\n","      <td>noise</td>\n","      <td>speech</td>\n","      <td>condition</td>\n","      <td>noisy</td>\n","      <td>enhancement</td>\n","      <td>clean</td>\n","      <td>signal</td>\n","      <td>environment</td>\n","      <td>background</td>\n","      <td>hour</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 6</th>\n","      <td>untranscribed</td>\n","      <td>gan</td>\n","      <td>pack</td>\n","      <td>randomly</td>\n","      <td>computer</td>\n","      <td>vocalisation</td>\n","      <td>limited</td>\n","      <td>affective</td>\n","      <td>investigate</td>\n","      <td>assign</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 7</th>\n","      <td>articulatory</td>\n","      <td>acoustic</td>\n","      <td>speech</td>\n","      <td>use</td>\n","      <td>tongue</td>\n","      <td>production</td>\n","      <td>articulation</td>\n","      <td>movement</td>\n","      <td>analysis</td>\n","      <td>time</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 8</th>\n","      <td>separation</td>\n","      <td>mixture</td>\n","      <td>music</td>\n","      <td>resolution</td>\n","      <td>magnitude</td>\n","      <td>variational</td>\n","      <td>seq</td>\n","      <td>identification</td>\n","      <td>temporal</td>\n","      <td>blind</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 9</th>\n","      <td>assistant</td>\n","      <td>smart</td>\n","      <td>home</td>\n","      <td>worker</td>\n","      <td>streaming</td>\n","      <td>biometric</td>\n","      <td>deployment</td>\n","      <td>aggregation</td>\n","      <td>vulnerable</td>\n","      <td>mediate</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 10</th>\n","      <td>duration</td>\n","      <td>consonant</td>\n","      <td>vowel</td>\n","      <td>study</td>\n","      <td>result</td>\n","      <td>character</td>\n","      <td>contrast</td>\n","      <td>japanese</td>\n","      <td>stop</td>\n","      <td>produce</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 11</th>\n","      <td>emotion</td>\n","      <td>emotional</td>\n","      <td>speech</td>\n","      <td>behavior</td>\n","      <td>recognition</td>\n","      <td>expression</td>\n","      <td>state</td>\n","      <td>neutral</td>\n","      <td>valence</td>\n","      <td>study</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 12</th>\n","      <td>speaker</td>\n","      <td>localization</td>\n","      <td>estimation</td>\n","      <td>use</td>\n","      <td>concurrent</td>\n","      <td>extension</td>\n","      <td>algorithm</td>\n","      <td>world</td>\n","      <td>wise</td>\n","      <td>reverberant</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 13</th>\n","      <td>model</td>\n","      <td>speech</td>\n","      <td>recognition</td>\n","      <td>time</td>\n","      <td>sequence</td>\n","      <td>encoder</td>\n","      <td>decoder</td>\n","      <td>phone</td>\n","      <td>search</td>\n","      <td>keyword</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 14</th>\n","      <td>phonetic</td>\n","      <td>word</td>\n","      <td>phone</td>\n","      <td>question</td>\n","      <td>decision</td>\n","      <td>rule</td>\n","      <td>tree</td>\n","      <td>similarity</td>\n","      <td>context</td>\n","      <td>token</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 15</th>\n","      <td>feature</td>\n","      <td>use</td>\n","      <td>classification</td>\n","      <td>set</td>\n","      <td>challenge</td>\n","      <td>classifier</td>\n","      <td>level</td>\n","      <td>base</td>\n","      <td>detection</td>\n","      <td>accuracy</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 16</th>\n","      <td>ctc</td>\n","      <td>cqcc</td>\n","      <td>reddot</td>\n","      <td>trivial</td>\n","      <td>wrong</td>\n","      <td>fundamental</td>\n","      <td>identity</td>\n","      <td>morph</td>\n","      <td>selectively</td>\n","      <td>foster</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 17</th>\n","      <td>sentence</td>\n","      <td>spoken</td>\n","      <td>semantic</td>\n","      <td>random</td>\n","      <td>query</td>\n","      <td>conditional</td>\n","      <td>phrase</td>\n","      <td>slot</td>\n","      <td>field</td>\n","      <td>understanding</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 18</th>\n","      <td>model</td>\n","      <td>propose</td>\n","      <td>use</td>\n","      <td>training</td>\n","      <td>speech</td>\n","      <td>base</td>\n","      <td>method</td>\n","      <td>approach</td>\n","      <td>datum</td>\n","      <td>performance</td>\n","    </tr>\n","    <tr>\n","      <th>Topic 19</th>\n","      <td>speech</td>\n","      <td>tone</td>\n","      <td>prosodic</td>\n","      <td>syllable</td>\n","      <td>pitch</td>\n","      <td>boundary</td>\n","      <td>prosody</td>\n","      <td>style</td>\n","      <td>level</td>\n","      <td>stress</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a698a09c-3e62-4a47-b8ed-34881bb82bb1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a698a09c-3e62-4a47-b8ed-34881bb82bb1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a698a09c-3e62-4a47-b8ed-34881bb82bb1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                 Word 0         Word 1  ...         Word 8         Word 9\n","Topic 0         angular  inconsistency  ...     disfluency     concurrent\n","Topic 1            unit        segment  ...       forensic       discover\n","Topic 2           human           user  ...       automate      switching\n","Topic 3           vowel     perception  ...       stimulus     experiment\n","Topic 4           datum            use  ...  transcription          paper\n","Topic 5           noise         speech  ...     background           hour\n","Topic 6   untranscribed            gan  ...    investigate         assign\n","Topic 7    articulatory       acoustic  ...       analysis           time\n","Topic 8      separation        mixture  ...       temporal          blind\n","Topic 9       assistant          smart  ...     vulnerable        mediate\n","Topic 10       duration      consonant  ...           stop        produce\n","Topic 11        emotion      emotional  ...        valence          study\n","Topic 12        speaker   localization  ...           wise    reverberant\n","Topic 13          model         speech  ...         search        keyword\n","Topic 14       phonetic           word  ...        context          token\n","Topic 15        feature            use  ...      detection       accuracy\n","Topic 16            ctc           cqcc  ...    selectively         foster\n","Topic 17       sentence         spoken  ...          field  understanding\n","Topic 18          model        propose  ...          datum    performance\n","Topic 19         speech           tone  ...          level         stress\n","\n","[20 rows x 10 columns]"]},"metadata":{},"execution_count":9}]}]}